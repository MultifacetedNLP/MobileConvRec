{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n%env CUDA_VISIBLE_DEVICES=0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model uses two pre-trained models inside it.\n\nmodel1:\n\n    previous_interactions_model (pre-trained model available and code to train it provided)\n    \nmodel2:\n\n    conversation_model (pre-trained model in baselines \"gpt2 recommender\")","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\nimport torch\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom dataclasses import dataclass\nfrom typing import Optional, List\nfrom fuzzywuzzy import fuzz\nimport numpy as np\nimport csv\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom typing import Optional, Tuple, Union\nfrom torch.nn import CrossEntropyLoss\nfrom transformers.modeling_outputs import CausalLMOutputWithCrossAttentions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Training and Validation Data","metadata":{}},{"cell_type":"code","source":"train_raw = pd.read_json(\"training data json file path\", lines=True)\nvalid_raw = pd.read_json(\"validation data json file path\", lines=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_approximate_substring(substring, string, threshold=70):\n    for i in range(len(string) - len(substring) + 1):\n        window = string[i:i+len(substring)]\n        similarity_ratio = fuzz.ratio(substring, window)\n        if similarity_ratio >= threshold:\n            return True\n    return False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_id = []\nprevious_interactions = []\nrecommended_app_name = []\nturns = []\nrecommend_indexes = []\n\nfor index, row in train_raw.iterrows():\n    user_id.append(row['user_id'])\n    prev = row['user_previous_interactions']\n    prev_apps = [app['app_name'] for app in prev]\n    if len(prev_apps) > 0:\n        previous_interactions.append(\",\".join(prev_apps))\n    else:\n        previous_interactions.append(None)\n    recommended_app_name.append(row['recommended_app']['app_name'])\n    dialog_turns = []\n    dialog_index = 0\n    found_index = False\n    for conv in row['turns']:\n        if \"COMPUTER\" in conv:\n            turn = 'computer: '+conv['COMPUTER']\n            if (row['recommended_app']['app_name'] in turn) and not found_index:\n                recommend_indexes.append(dialog_index)\n                found_index = True\n            dialog_turns.append(turn)\n            dialog_index+=1\n        if \"HUMAN\" in conv:\n            turn = 'human: '+conv['HUMAN']\n            dialog_turns.append(turn)\n            dialog_index+=1\n    if not found_index: # approximately finding the recommender turn\n        for i, dialog_turn in enumerate(dialog_turns):\n            if is_approximate_substring(row['recommended_app']['app_name'], dialog_turn):\n                recommend_indexes.append(i)\n                found_index = True\n                break\n                    \n    if not found_index:\n        recommend_indexes.append(-1)\n                        \n    turns.append(dialog_turns)\n    \nprint(len(user_id))\nprint(len(previous_interactions))\nprint(len(recommended_app_name))\nprint(len(recommend_indexes))\ndf_recommender_train = pd.DataFrame({\"user_id\": user_id, \"previous_interactions\":previous_interactions, \"recommended_app_name\":recommended_app_name, \"turns\": turns, \"recommend_indexes\":recommend_indexes})\nprint(f\"\\nnumber of rows: {len(df_recommender_train)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_id = []\nprevious_interactions = []\nrecommended_app_name = []\nturns = []\nrecommend_indexes = []\n\nfor index, row in valid_raw.iterrows():\n    user_id.append(row['user_id'])\n    prev = row['user_previous_interactions']\n    prev_apps = [app['app_name'] for app in prev]\n    if len(prev_apps) > 0:\n        previous_interactions.append(\",\".join(prev_apps))\n    else:\n        previous_interactions.append(None)\n    recommended_app_name.append(row['recommended_app']['app_name'])\n    dialog_turns = []\n    dialog_index = 0\n    found_index = False\n    for conv in row['turns']:\n        if \"COMPUTER\" in conv:\n            turn = 'computer: '+conv['COMPUTER']\n            if (row['recommended_app']['app_name'] in turn) and not found_index:\n                recommend_indexes.append(dialog_index)\n                found_index = True\n            dialog_turns.append(turn)\n            dialog_index+=1\n        if \"HUMAN\" in conv:\n            turn = 'human: '+conv['HUMAN']\n            dialog_turns.append(turn)\n            dialog_index+=1\n    if not found_index: # approximately finding the recommender turn\n        for i, dialog_turn in enumerate(dialog_turns):\n            if is_approximate_substring(row['recommended_app']['app_name'], dialog_turn):\n                recommend_indexes.append(i)\n                found_index = True\n                break\n                    \n    if not found_index:\n        recommend_indexes.append(-1)\n                        \n    turns.append(dialog_turns)\n    \nprint(len(user_id))\nprint(len(previous_interactions))\nprint(len(recommended_app_name))\nprint(len(recommend_indexes))\ndf_recommender_validation = pd.DataFrame({\"user_id\": user_id, \"previous_interactions\":previous_interactions, \"recommended_app_name\":recommended_app_name, \"turns\": turns, \"recommend_indexes\":recommend_indexes})\nprint(f\"\\nnumber of rows: {len(df_recommender_validation)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_recommender_train = df_recommender_train[(df_recommender_train[\"recommend_indexes\"] != -1) & (df_recommender_train[\"turns\"].apply(lambda x: len(x) > 0))]\ndf_recommender_train['user_id'] = df_recommender_train['user_id'].str.lower()\ndf_recommender_train['previous_interactions'] = df_recommender_train['previous_interactions'].str.lower()\ndf_recommender_train['recommended_app_name'] = df_recommender_train['recommended_app_name'].str.lower()\ndf_recommender_train['turns'] = df_recommender_train['turns'].apply(lambda x: [s.lower() for s in x])\n\ndf_recommender_validation = df_recommender_validation[(df_recommender_validation[\"recommend_indexes\"] != -1) & (df_recommender_validation[\"turns\"].apply(lambda x: len(x) > 0))]\ndf_recommender_validation['user_id'] = df_recommender_validation['user_id'].str.lower()\ndf_recommender_validation['previous_interactions'] = df_recommender_validation['previous_interactions'].str.lower()\ndf_recommender_validation['recommended_app_name'] = df_recommender_validation['recommended_app_name'].str.lower()\ndf_recommender_validation['turns'] = df_recommender_validation['turns'].apply(lambda x: [s.lower() for s in x])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Definition","metadata":{}},{"cell_type":"markdown","source":"model1 is previous interactions model\n\nmodel2 is conversation model","metadata":{}},{"cell_type":"code","source":"model1_path = \"model1 path\"\nmodel2_path = \"model2 path\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GPT2Encoder(nn.Module):\n    def __init__(self, model_name='gpt2'):\n        super(GPT2Encoder, self).__init__()\n        self.transformer = GPT2LMHeadModel.from_pretrained(model_name).transformer\n\n    def forward(self, input_ids, attention_mask=None):\n        return self.transformer(input_ids, attention_mask=attention_mask)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv1D(nn.Module):\n    \"\"\"\n    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n    Basically works like a linear layer but the weights are transposed.\n    Args:\n        nf (`int`): The number of output features.\n        nx (`int`): The number of input features.\n    \"\"\"\n    def __init__(self, nf, nx):\n        super().__init__()\n        self.nf = nf\n        self.weight = nn.Parameter(torch.empty(nx, nf))\n        self.bias = nn.Parameter(torch.zeros(nf))\n        nn.init.normal_(self.weight, std=0.02)\n\n    def forward(self, x):\n        size_out = x.size()[:-1] + (self.nf,)\n        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n        x = x.view(size_out)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, max_position_embeddings):\n        super().__init__()\n        self.embed_dim = hidden_size\n        self.num_heads = num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        self.split_size = self.embed_dim\n        \n        if self.head_dim * self.num_heads != self.embed_dim:\n            raise ValueError(f\"`embed_dim` must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {self.num_heads}).\")\n\n        self.c_attn = Conv1D(2 * self.embed_dim, self.embed_dim)\n        self.q_attn = Conv1D(self.embed_dim, self.embed_dim)\n        self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n\n        self.attn_dropout = nn.Dropout(0.1)\n        self.resid_dropout = nn.Dropout(0.1)\n\n        self.register_buffer(\"bias\", torch.tril(torch.ones((max_position_embeddings, max_position_embeddings), dtype=torch.bool)).view(1, 1, max_position_embeddings, max_position_embeddings), persistent=False)\n        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n\n    def _attn(self, query, key, value, attention_mask_kv=None, head_mask=None):\n        attn_weights = torch.matmul(query, key.transpose(-1, -2)) #(batch,n_head,seqlen_q,seqlen_kv)\n        attn_weights = attn_weights / (value.size(-1) ** 0.5)\n\n        if attention_mask_kv is not None:\n            # Attention mask for key-value pairs  # Shape: (batch_size, 1, 1, seq_length_kv)\n            attn_weights = attn_weights + attention_mask_kv #attention mask is (0,-inf)\n\n        \n\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n        attn_weights = attn_weights.type(value.dtype)\n        attn_weights = self.attn_dropout(attn_weights)\n        \n        if head_mask is not None:\n            attn_weights = attn_weights * head_mask\n\n        attn_output = torch.matmul(attn_weights, value) #(batch,n_head,seqlen_q,head_dim)\n        return attn_output, attn_weights\n\n    def _split_heads(self, tensor, num_heads, attn_head_size):\n        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n        tensor = tensor.view(new_shape)\n        return tensor.permute(0, 2, 1, 3)\n\n    def _merge_heads(self, tensor, num_heads, attn_head_size):\n        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n        return tensor.view(new_shape)\n\n    def forward(\n        self,\n        hidden_states: torch.FloatTensor,\n        encoder_hidden_states: torch.Tensor,\n        attention_mask_kv: Optional[torch.FloatTensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        query = self.q_attn(hidden_states) \n        key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n\n        query = self._split_heads(query, self.num_heads, self.head_dim) #(batch,n_head,sequence_len,head_dim)\n        key = self._split_heads(key, self.num_heads, self.head_dim) #(batch,n_head,sequence_len,head_dim)\n        value = self._split_heads(value, self.num_heads, self.head_dim) #(batch,n_head,sequence_len,head_dim)\n        #attention masks shape (batch,1,1,sequence_len)\n        \n\n        attn_output, attn_weights = self._attn(query, key, value, attention_mask_kv)\n        \n\n        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)#(batch,seqlen,n_embd)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output)\n\n        return attn_output, attn_weights\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GPT2MLP(nn.Module):\n    def __init__(self, embed_dim,multiplyer):\n        super().__init__()\n        intermediate_size = multiplyer * embed_dim\n        self.c_fc = Conv1D(intermediate_size, embed_dim)\n        self.c_proj = Conv1D(embed_dim, intermediate_size)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n        hidden_states = self.c_fc(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.c_proj(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        return hidden_states","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, n_embd, n_head,max_length):\n        super(TransformerBlock, self).__init__()\n        self.attention = CrossAttention(n_embd, n_head,max_length)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.feed_forward = GPT2MLP(n_embd,4)\n        self.ln2 = nn.LayerNorm(n_embd)\n        self.ln3 = nn.LayerNorm(n_embd)\n\n    def forward(self, x,context,attention_mask):\n        x = self.ln2(x)\n        context = self.ln3(x)\n        y,z = self.attention(x,context,attention_mask)\n        x = x+y\n        residual = x\n        x = self.ln1(x)\n        x = residual + self.feed_forward(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpt2_model = GPT2LMHeadModel.from_pretrained(model2_path)\ngpt2_model.resize_token_embeddings(len(tokenizer))\npretrained_linear_layer = gpt2_model.lm_head","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CombinedModel(nn.Module):\n    def __init__(self, model1 = '/kaggle/working/models/model1',model2 = '/kaggle/input/gp2-recommender/GPT2_recommender', n_head=12,n_layer=12,n_embd=768,max_length=512):\n        super(CombinedModel, self).__init__()\n        self.encoder1 = GPT2Encoder(model1)\n        self.encoder2 = GPT2Encoder(model2)\n        self.h = nn.ModuleList([TransformerBlock(n_embd, n_head, max_length) for i in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = pretrained_linear_layer\n        self.loss_fn = CrossEntropyLoss(ignore_index=-100)\n\n    def forward(self, input_ids1, input_ids2, attention_mask1=None, attention_mask2=None,labels=None):\n        # Encode inputs using both encoders\n        encoded_output1 = self.encoder1(input_ids1, attention_mask1).last_hidden_state\n        encoded_output2 = self.encoder2(input_ids2, attention_mask2).last_hidden_state\n        x = encoded_output2\n\n        # Apply cross-attention\n        for layer in self.h:\n            x = layer(x,encoded_output1,attention_mask1)\n            \n        normalized = self.ln_f(x)\n        \n        \n        final_output = self.lm_head(normalized)\n        \n        loss = None\n        if labels is not None:\n            # Shift labels and final_output to the right to align with prediction\n            shift_logits = final_output[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n            \n        if loss is not None:\n            return CausalLMOutputWithCrossAttentions(loss=loss, logits=final_output)\n        return CausalLMOutputWithCrossAttentions(logits=final_output)\n    \n    \n    \n    def get_last_non_padding_token_position(input_ids, pad_token_id):\n        # Get the positions of the last non-padding tokens\n        non_pad_positions = (input_ids != pad_token_id).nonzero(as_tuple=True)[1]\n        last_non_pad_position = non_pad_positions[-1]\n        return last_non_pad_position\n    \n    \n    def generate(self, input_ids1, input_ids2, attention_mask1=None, attention_mask2=None, max_length=20, temperature=1.0, tokenizer=None):\n        if tokenizer is None:\n            raise ValueError(\"Tokenizer must be provided\")\n\n        eos_token_id = tokenizer.eos_token_id\n        pad_token_id = tokenizer.pad_token_id\n\n        # Ensure the model is in evaluation mode\n        self.eval()\n        if input_ids1.dim() == 1:\n            input_ids1 = input_ids1.unsqueeze(0)\n        if input_ids2.dim() == 1:\n            input_ids2 = input_ids2.unsqueeze(0)\n        if attention_mask1 is not None and attention_mask1.dim() == 1:\n            attention_mask1 = attention_mask1.unsqueeze(0)\n        if attention_mask2 is not None and attention_mask2.dim() == 1:\n            attention_mask2 = attention_mask2.unsqueeze(0)\n\n        generated_sequence = []\n\n        with torch.no_grad():\n            for _ in range(max_length):\n                pos = (input_ids2 != pad_token_id).nonzero(as_tuple=True)[1][-1].item()\n                output = self.forward(input_ids1, input_ids2, attention_mask1, attention_mask2)\n                logits = output.logits[:, pos, :] / temperature\n                probabilities = torch.nn.functional.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probabilities, num_samples=1)\n\n                # Append the generated tokens to the respective sequences\n                generated_sequence.append(next_token.item())\n\n                # Break the loop if the EOS token is generated for all sequences\n                if (next_token.item() == eos_token_id):\n                    break\n\n                # Update input_ids2 and attention_mask2 by appending the new token\n                \n                input_ids2 = torch.cat([input_ids2[:, :pos + 1], next_token,input_ids2[:, pos + 2:]], dim=1)\n                attention_mask2 = torch.cat([attention_mask2[:, :pos + 1], torch.ones((1, 1), device=input_ids2.device),attention_mask2[:, pos + 2:]], dim=1)\n\n\n        return generated_sequence\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model1 here is previous interactions model\nModel2 here is conversation model","metadata":{}},{"cell_type":"code","source":"model_max_length=512\nmodel = CombinedModel(model1=model1_path,model2=model2_path,n_head=12,n_layer=12,n_embd=768,max_length=model_max_length)\nprint(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"gpt2\"\nbos = '<|startoftext|>'\neos = '<|endoftext|>'\npad = '<|pad|>'\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint, bos_token=bos, eos_token=eos, pad_token=pad, additional_special_tokens=[\"computer:\", \"human:\"])\n\nprint(len(tokenizer))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass RecommenderItem:\n    prompt: str\n    generation: Optional[str] = None\n    interaction: Optional[str] = None\n    \nclass recommenderDataset(Dataset):\n    def __init__(self, data: List[RecommenderItem]):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx) -> RecommenderItem:\n        return self.data[idx]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_train = []\nfor _, row in df_recommender_train.iterrows():\n    interactions = bos\n    prompt = bos\n    if row[\"previous_interactions\"] is not None:\n        interactions = interactions + row[\"previous_interactions\"]\n    else:\n        interactions = interactions + \"None\"\n    for index, turn in enumerate(row[\"turns\"]):\n        if index < row[\"recommend_indexes\"]:\n            prompt += turn + \"\\n\"\n        elif index == row[\"recommend_indexes\"]:\n            prompt += \"computer: I would recommend the \"\n            items_train.append(RecommenderItem(prompt, row[\"recommended_app_name\"] + \" app.\" + eos,interactions))\n            break\n        else:\n            print(\"error!!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_validation = []\nfor _, row in df_recommender_validation.iterrows():\n    interactions = bos\n    prompt = bos\n    if row[\"previous_interactions\"] is not None:\n        interactions = interactions + row[\"previous_interactions\"]\n    else:\n        interactions = interactions + \"None\"\n    for index, turn in enumerate(row[\"turns\"]):\n        if index < row[\"recommend_indexes\"]:\n            prompt += turn + \"\\n\"\n        elif index == row[\"recommend_indexes\"]:\n            prompt += \"computer: I would recommend the \"\n            items_validation.append(RecommenderItem(prompt, row[\"recommended_app_name\"] + \" app.\" + eos,interactions))\n            break\n        else:\n            print(\"error!!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_collator(batch: list[recommenderDataset]): # for training a language model\n    input_ids1 = []\n    attention_masks1 = []\n    input_ids2 = []\n    attention_masks2 = []\n    labels = []\n    for item in batch:\n        interaction_tokens = tokenizer.encode(item.interaction, return_tensors=\"pt\")[0]\n        prompt_tokens = tokenizer.encode(item.prompt, return_tensors=\"pt\")[0] \n        generation_tokens = tokenizer.encode(item.generation, return_tensors=\"pt\")[0]\n        prompt_len = len(prompt_tokens)\n        generation_len = len(generation_tokens)\n        interaction_len = len(interaction_tokens)\n        unused_len1 = model_max_length - prompt_len - generation_len\n        unused_len2 = model_max_length - interaction_len\n        # handling case when input is greater than tokenizer length.\n        \n        if unused_len2 < 0:\n            interaction_start_tokens = interaction_tokens[:1]\n            trimmed_interaction = interaction_tokens[unused_len2 * -1 + 1 :] # TODO: you could delete the prompt to reach the first |beginuser| token\n            interaction_tokens = torch.cat(\n                [interaction_start_tokens, trimmed_interaction], axis=0\n            )\n            prompt_len = len(prompt_tokens)\n            unused_len1 = 0\n        \n        if unused_len1 < 0:\n            prompt_start_tokens = prompt_tokens[:1]\n            trimmed_prompt = prompt_tokens[unused_len1 * -1 + 1 :] # TODO: you could delete the prompt to reach the first |beginuser| token\n            prompt_tokens = torch.cat(\n                [prompt_start_tokens, trimmed_prompt], axis=0\n            )\n            prompt_len = len(prompt_tokens)\n            unused_len1 = 0\n        pad1 = torch.full([unused_len1], tokenizer.pad_token_id)\n        pad2 = torch.full([unused_len2], tokenizer.pad_token_id)\n        input_tokens2 = torch.cat(\n            [interaction_tokens, pad2]\n        )\n        input_tokens1 = torch.cat(\n            [prompt_tokens, generation_tokens, pad1]\n        )\n        label = torch.cat(\n            [\n                torch.full(\n                    [prompt_len],\n                    -100,\n                ),\n                generation_tokens,\n                torch.full([unused_len1], -100),\n            ]\n        )\n        attention_mask1 = torch.cat(\n            [\n                torch.full([prompt_len + generation_len], 1),\n                torch.full([unused_len1], 0),\n            ]\n        )\n        attention_mask2 = torch.cat(\n            [\n                torch.full([interaction_len], 1),\n                torch.full([unused_len2], 0),\n            ]\n        )\n        input_ids1.append(input_tokens1)\n        attention_masks1.append(attention_mask1)\n        input_ids2.append(input_tokens2)\n        attention_masks2.append(attention_mask2)\n        labels.append(label)\n\n    out = {\n        \"input_ids2\": torch.stack(input_ids1),\n        \"attention_mask2\": torch.stack(attention_masks1),\n        \"input_ids1\": torch.stack(input_ids2),\n        \"attention_mask1\": torch.stack(attention_masks2),\n        \"labels\": torch.stack(labels),\n    }\n\n    return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"model save path\",\n    num_train_epochs=5,\n    # logging_steps=500,\n    # logging_dir=self.cfg.logging_dir,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    save_strategy=\"steps\",\n    evaluation_strategy=\"steps\",\n    save_steps=0.3,#self.cfg.save_steps,\n    eval_steps=0.3, #self.cfg.eval_steps,\n    save_total_limit=4,\n    gradient_accumulation_steps=3, #gradient_accumulation_steps,\n    per_device_train_batch_size=3, #train_batch_size,\n    per_device_eval_batch_size=3, #self.cfg.eval_batch_size,\n    warmup_steps=100,\n    weight_decay=0.01,\n    # dataloader_drop_last=True,\n    disable_tqdm=False,\n    report_to='none',\n    push_to_hub=False\n)\n\n\ntrainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=recommenderDataset(items_train),\n        eval_dataset=recommenderDataset(items_validation), #dm.datasets[DataNames.dev_language_model.value],\n        data_collator=training_collator,\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are using hugging face trainer and the model is not completely made for it so it might not be able to save using trainer.save_model()\n\nIn that case, use this","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"save path\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Test Data","metadata":{}},{"cell_type":"code","source":"test_raw = pd.read_json(\"Test data json file path\", lines=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_approximate_substring(substring, string, threshold=70):\n    for i in range(len(string) - len(substring) + 1):\n        window = string[i:i+len(substring)]\n        similarity_ratio = fuzz.ratio(substring, window)\n        if similarity_ratio >= threshold:\n            return True\n    return False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_id = []\nprevious_interactions = []\nrecommended_app_name = []\nturns = []\nrecommend_indexes = []\n\nfor index, row in test_raw.iterrows():\n    user_id.append(row['user_id'])\n    prev = row['user_previous_interactions']\n    prev_apps = [app['app_name'] for app in prev]\n    if len(prev_apps) > 0:\n        previous_interactions.append(\",\".join(prev_apps))\n    else:\n        previous_interactions.append(None)\n    recommended_app_name.append(row['recommended_app']['app_name'])\n    dialog_turns = []\n    dialog_index = 0\n    found_index = False\n    for conv in row['turns']:\n        if \"COMPUTER\" in conv:\n            turn = 'computer: '+conv['COMPUTER']\n            if (row['recommended_app']['app_name'] in turn) and not found_index:\n                recommend_indexes.append(dialog_index)\n                found_index = True\n            dialog_turns.append(turn)\n            dialog_index+=1\n        if \"HUMAN\" in conv:\n            turn = 'human: '+conv['HUMAN']\n            dialog_turns.append(turn)\n            dialog_index+=1\n    if not found_index: # approximately finding the recommender turn\n        for i, dialog_turn in enumerate(dialog_turns):\n            if is_approximate_substring(row['recommended_app']['app_name'], dialog_turn):\n                recommend_indexes.append(i)\n                found_index = True\n                break\n                    \n    if not found_index:\n        recommend_indexes.append(-1)\n                        \n    turns.append(dialog_turns)\n    \nprint(len(user_id))\nprint(len(previous_interactions))\nprint(len(recommended_app_name))\nprint(len(recommend_indexes))\ndf_recommender_test = pd.DataFrame({\"user_id\": user_id, \"previous_interactions\":previous_interactions, \"recommended_app_name\":recommended_app_name, \"turns\": turns, \"recommend_indexes\":recommend_indexes})\nprint(f\"\\nnumber of rows: {len(df_recommender_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_recommender_test = df_recommender_test[(df_recommender_test[\"recommend_indexes\"] != -1) & (df_recommender_test[\"turns\"].apply(lambda x: len(x) > 0))]\ndf_recommender_test['user_id'] = df_recommender_test['user_id'].str.lower()\ndf_recommender_test['previous_interactions'] = df_recommender_test['previous_interactions'].str.lower()\ndf_recommender_test['recommended_app_name'] = df_recommender_test['recommended_app_name'].str.lower()\ndf_recommender_test['turns'] = df_recommender_test['turns'].apply(lambda x: [s.lower() for s in x])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Model","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"gpt2\"\nbos = '<|startoftext|>'\neos = '<|endoftext|>'\npad = '<|pad|>'\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint, bos_token=bos, eos_token=eos, pad_token=pad, additional_special_tokens=[\"computer:\", \"human:\"])\n\nprint(len(tokenizer))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"save path\"))\nmodel = model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_test = []\nfor _, row in df_recommender_test.iterrows():\n    interactions = bos\n    prompt = bos\n    if row[\"previous_interactions\"] is not None:\n        interactions = interactions + row[\"previous_interactions\"]\n    else:\n        interactions = interactions + \"None\"\n    for index, turn in enumerate(row[\"turns\"]):\n        if index < row[\"recommend_indexes\"]:\n            prompt += turn + \"\\n\"\n        elif index == row[\"recommend_indexes\"]:\n            prompt += \"computer: I would recommend the \"\n            items_test.append(RecommenderItem(prompt, row[\"recommended_app_name\"] + \" app.\",interactions))\n            break\n        else:\n            print(\"error!!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i:i+batch_size]\n\ndef evaluate_recommender(dataset, model, tokenizer, device=device, threshold=70):\n    batch_size=1\n    prompt_arr = [data.prompt for data in dataset]\n    generation_arr = [data.generation for data in dataset]\n    interaction_arr = [data.interaction for data in dataset]\n    max_length=480\n    generation_length = 32\n    print(len(dataset))\n    correctly_predicted = []\n    \n    for prompt, generation,interaction in tqdm(zip(prompt_arr, generation_arr,interaction_arr), total = len(generation_arr)):\n        \n        inputs1 = tokenizer(interaction,max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n        inputs2 = tokenizer(prompt,max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n\n        generations_predicted = model.generate(input_ids1=inputs1[\"input_ids\"].to(device),input_ids2=inputs2[\"input_ids\"].to(device), attention_mask1=inputs1[\"attention_mask\"].to(device),attention_mask2=inputs2[\"attention_mask\"].to(device),\n                            max_length=generation_length,\n                            tokenizer=tokenizer) # length_penalty=0.8, Set length_penalty to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer sequences.\n\n        generations_predicted = generations_predicted # we only need the generation part, not the prompt part.\n        decoded_generation = tokenizer.decode(generations_predicted, skip_special_tokens=True, clean_up_tokenization_spaces=True).replace(\" app.\", \"\")\n        generation = generation.replace(\" app.\", \"\")\n    \n        correctly_predicted.append(1 if fuzz.ratio(decoded_generation, generation) > threshold else 0)\n\n\n    return correctly_predicted","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generate Function does not currently work with multiple batches. I will update code with updated generate function.","metadata":{}},{"cell_type":"code","source":"correctly_predicted = evaluate_recommender(recommenderDataset(items_test), model, tokenizer, device=device, threshold=95)\nsuccess_rate = sum(correctly_predicted) / len(correctly_predicted)\nprint(\"success_rate1: \", success_rate)","metadata":{},"execution_count":null,"outputs":[]}]}