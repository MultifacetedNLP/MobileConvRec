{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n%env CUDA_VISIBLE_DEVICES=0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\nimport torch\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom dataclasses import dataclass\nfrom typing import Optional, List\nfrom fuzzywuzzy import fuzz\nimport numpy as np\nimport csv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Training and Validation Data","metadata":{}},{"cell_type":"code","source":"train_raw = pd.read_json(\"training data json file path\", lines=True)\nvalid_raw = pd.read_json(\"validation data json file path\", lines=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_approximate_substring(substring, string, threshold=70):\n    for i in range(len(string) - len(substring) + 1):\n        window = string[i:i+len(substring)]\n        similarity_ratio = fuzz.ratio(substring, window)\n        if similarity_ratio >= threshold:\n            return True\n    return False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_id = []\nprevious_interactions = []\nrecommended_app_name = []\nturns = []\nrecommend_indexes = []\n\nfor index, row in train_raw.iterrows():\n    user_id.append(row['user_id'])\n    prev = row['user_previous_interactions']\n    prev_apps = [app['app_name'] for app in prev]\n    if len(prev_apps) > 0:\n        previous_interactions.append(\",\".join(prev_apps))\n    else:\n        previous_interactions.append(None)\n    recommended_app_name.append(row['recommended_app']['app_name'])\n    dialog_turns = []\n    dialog_index = 0\n    found_index = False\n    for conv in row['turns']:\n        if \"COMPUTER\" in conv:\n            turn = 'computer: '+conv['COMPUTER']\n            if (row['recommended_app']['app_name'] in turn) and not found_index:\n                recommend_indexes.append(dialog_index)\n                found_index = True\n            dialog_turns.append(turn)\n            dialog_index+=1\n        if \"HUMAN\" in conv:\n            turn = 'human: '+conv['HUMAN']\n            dialog_turns.append(turn)\n            dialog_index+=1\n    if not found_index: # approximately finding the recommender turn\n        for i, dialog_turn in enumerate(dialog_turns):\n            if is_approximate_substring(row['recommended_app']['app_name'], dialog_turn):\n                recommend_indexes.append(i)\n                found_index = True\n                break\n                    \n    if not found_index:\n        recommend_indexes.append(-1)\n                        \n    turns.append(dialog_turns)\n    \nprint(len(user_id))\nprint(len(previous_interactions))\nprint(len(recommended_app_name))\nprint(len(recommend_indexes))\ndf_recommender_train = pd.DataFrame({\"user_id\": user_id, \"previous_interactions\":previous_interactions, \"recommended_app_name\":recommended_app_name, \"turns\": turns, \"recommend_indexes\":recommend_indexes})\nprint(f\"\\nnumber of rows: {len(df_recommender_train)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_id = []\nprevious_interactions = []\nrecommended_app_name = []\nturns = []\nrecommend_indexes = []\n\nfor index, row in valid_raw.iterrows():\n    user_id.append(row['user_id'])\n    prev = row['user_previous_interactions']\n    prev_apps = [app['app_name'] for app in prev]\n    if len(prev_apps) > 0:\n        previous_interactions.append(\",\".join(prev_apps))\n    else:\n        previous_interactions.append(None)\n    recommended_app_name.append(row['recommended_app']['app_name'])\n    dialog_turns = []\n    dialog_index = 0\n    found_index = False\n    for conv in row['turns']:\n        if \"COMPUTER\" in conv:\n            turn = 'computer: '+conv['COMPUTER']\n            if (row['recommended_app']['app_name'] in turn) and not found_index:\n                recommend_indexes.append(dialog_index)\n                found_index = True\n            dialog_turns.append(turn)\n            dialog_index+=1\n        if \"HUMAN\" in conv:\n            turn = 'human: '+conv['HUMAN']\n            dialog_turns.append(turn)\n            dialog_index+=1\n    if not found_index: # approximately finding the recommender turn\n        for i, dialog_turn in enumerate(dialog_turns):\n            if is_approximate_substring(row['recommended_app']['app_name'], dialog_turn):\n                recommend_indexes.append(i)\n                found_index = True\n                break\n                    \n    if not found_index:\n        recommend_indexes.append(-1)\n                        \n    turns.append(dialog_turns)\n    \nprint(len(user_id))\nprint(len(previous_interactions))\nprint(len(recommended_app_name))\nprint(len(recommend_indexes))\ndf_recommender_validation = pd.DataFrame({\"user_id\": user_id, \"previous_interactions\":previous_interactions, \"recommended_app_name\":recommended_app_name, \"turns\": turns, \"recommend_indexes\":recommend_indexes})\nprint(f\"\\nnumber of rows: {len(df_recommender_validation)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_recommender_train = df_recommender_train[(df_recommender_train[\"recommend_indexes\"] != -1) & (df_recommender_train[\"turns\"].apply(lambda x: len(x) > 0))]\ndf_recommender_train['user_id'] = df_recommender_train['user_id'].str.lower()\ndf_recommender_train['previous_interactions'] = df_recommender_train['previous_interactions'].str.lower()\ndf_recommender_train['recommended_app_name'] = df_recommender_train['recommended_app_name'].str.lower()\ndf_recommender_train['turns'] = df_recommender_train['turns'].apply(lambda x: [s.lower() for s in x])\n\ndf_recommender_validation = df_recommender_validation[(df_recommender_validation[\"recommend_indexes\"] != -1) & (df_recommender_validation[\"turns\"].apply(lambda x: len(x) > 0))]\ndf_recommender_validation['user_id'] = df_recommender_validation['user_id'].str.lower()\ndf_recommender_validation['previous_interactions'] = df_recommender_validation['previous_interactions'].str.lower()\ndf_recommender_validation['recommended_app_name'] = df_recommender_validation['recommended_app_name'].str.lower()\ndf_recommender_validation['turns'] = df_recommender_validation['turns'].apply(lambda x: [s.lower() for s in x])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"markdown","source":"This model is for use in combined models. Two versions of it get trained to match sequence length and tokenizer vocabolary to avoid complications in all conditions.\n\nVer1:\n\n    special_tokens = [\"computer:\", \"human:\"], max_length = 512\n    \nVer2:\n\n    special_tokens = [\"computer:\", \"human:\", \"candidate_apps:\"], max_length = 512\n    \nI have commented out parts to change in case of ver2","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"gpt2\"\nbos = '<|startoftext|>'\neos = '<|endoftext|>'\npad = '<|pad|>'\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint, bos_token=bos, eos_token=eos, pad_token=pad, additional_special_tokens=[\"computer:\", \"human:\"])\n#tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint, bos_token=bos, eos_token=eos, pad_token=pad, additional_special_tokens=[\"computer:\", \"human:\", \"candidate_apps:\"])\nmodel = GPT2LMHeadModel.from_pretrained(model_checkpoint).to(device)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel_max_length=512\n#model_max_length=1024","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass RecommenderItem:\n    prompt: str\n    generation: Optional[str] = None\n    \nclass recommenderDataset(Dataset):\n    def __init__(self, data: List[RecommenderItem]):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx) -> RecommenderItem:\n        return self.data[idx]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_train = []\nfor _, row in df_recommender_train.iterrows():\n    if row[\"previous_interactions\"] is not None:\n        prompt = bos + row[\"previous_interactions\"]\n    else:\n        prompt = bos + \"None\"\n    items_train.append(RecommenderItem(prompt, ', ' + row[\"recommended_app_name\"] + eos))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_validation = []\nfor _, row in df_recommender_validation.iterrows():\n    if row[\"previous_interactions\"] is not None:\n        prompt = bos + row[\"previous_interactions\"]\n    else:\n        prompt = bos + \"None\"\n    items_validation.append(RecommenderItem(prompt, ', ' + row[\"recommended_app_name\"] + eos))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_collator(batch: list[recommenderDataset]): # for training a language model\n    input_ids = []\n    attention_masks = []\n    labels = []\n    for item in batch:\n        prompt_tokens = tokenizer.encode(item.prompt, return_tensors=\"pt\")[0] \n        generation_tokens = tokenizer.encode(item.generation, return_tensors=\"pt\")[0]\n        prompt_len = len(prompt_tokens)\n        generation_len = len(generation_tokens)\n        unused_len = model_max_length - prompt_len - generation_len\n        # handling case when input is greater than tokenizer length.\n        if unused_len < 0:\n            prompt_start_tokens = prompt_tokens[:1]\n            trimmed_prompt = prompt_tokens[unused_len * -1 + 1 :] # TODO: you could delete the prompt to reach the first |beginuser| token\n            prompt_tokens = torch.cat(\n                [prompt_start_tokens, trimmed_prompt], axis=0\n            )\n            prompt_len = len(prompt_tokens)\n            unused_len = 0\n        pad = torch.full([unused_len], tokenizer.pad_token_id)\n        input_tokens = torch.cat(\n            [prompt_tokens, generation_tokens, pad]\n        )\n        label = torch.cat(\n            [\n                torch.full(\n                    [prompt_len],\n                    -100,\n                ),\n                generation_tokens,\n                torch.full([unused_len], -100),\n            ]\n        )\n        attention_mask = torch.cat(\n            [\n                torch.full([prompt_len + generation_len], 1),\n                torch.full([unused_len], 0),\n            ]\n        )\n        input_ids.append(input_tokens)\n        attention_masks.append(attention_mask)\n        labels.append(label)\n\n    out = {\n        \"input_ids\": torch.stack(input_ids),\n        \"attention_mask\": torch.stack(attention_masks),\n        \"labels\": torch.stack(labels),\n    }\n\n    return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"model save path\",\n    num_train_epochs=5,\n    # logging_steps=500,\n    # logging_dir=self.cfg.logging_dir,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    save_strategy=\"steps\",\n    evaluation_strategy=\"steps\",\n    save_steps=0.3,#self.cfg.save_steps,\n    eval_steps=0.3, #self.cfg.eval_steps,\n    save_total_limit=4,\n    gradient_accumulation_steps=3, #gradient_accumulation_steps,\n    per_device_train_batch_size=4, #train_batch_size,\n    per_device_eval_batch_size=4, #self.cfg.eval_batch_size,\n    warmup_steps=100,\n    weight_decay=0.01,\n    # dataloader_drop_last=True,\n    disable_tqdm=False,\n    report_to='none',\n    push_to_hub=False\n)\n\n\ntrainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=recommenderDataset(items_train),\n        eval_dataset=recommenderDataset(items_validation), #dm.datasets[DataNames.dev_language_model.value],\n        data_collator=training_collator,\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model()\ntorch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Test Data","metadata":{}},{"cell_type":"code","source":"test_raw = pd.read_json(\"Test data json file path\", lines=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_approximate_substring(substring, string, threshold=70):\n    for i in range(len(string) - len(substring) + 1):\n        window = string[i:i+len(substring)]\n        similarity_ratio = fuzz.ratio(substring, window)\n        if similarity_ratio >= threshold:\n            return True\n    return False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_id = []\nprevious_interactions = []\nrecommended_app_name = []\nturns = []\nrecommend_indexes = []\n\nfor index, row in test_raw.iterrows():\n    user_id.append(row['user_id'])\n    prev = row['user_previous_interactions']\n    prev_apps = [app['app_name'] for app in prev]\n    if len(prev_apps) > 0:\n        previous_interactions.append(\",\".join(prev_apps))\n    else:\n        previous_interactions.append(None)\n    recommended_app_name.append(row['recommended_app']['app_name'])\n    dialog_turns = []\n    dialog_index = 0\n    found_index = False\n    for conv in row['turns']:\n        if \"COMPUTER\" in conv:\n            turn = 'computer: '+conv['COMPUTER']\n            if (row['recommended_app']['app_name'] in turn) and not found_index:\n                recommend_indexes.append(dialog_index)\n                found_index = True\n            dialog_turns.append(turn)\n            dialog_index+=1\n        if \"HUMAN\" in conv:\n            turn = 'human: '+conv['HUMAN']\n            dialog_turns.append(turn)\n            dialog_index+=1\n    if not found_index: # approximately finding the recommender turn\n        for i, dialog_turn in enumerate(dialog_turns):\n            if is_approximate_substring(row['recommended_app']['app_name'], dialog_turn):\n                recommend_indexes.append(i)\n                found_index = True\n                break\n                    \n    if not found_index:\n        recommend_indexes.append(-1)\n                        \n    turns.append(dialog_turns)\n    \nprint(len(user_id))\nprint(len(previous_interactions))\nprint(len(recommended_app_name))\nprint(len(recommend_indexes))\ndf_recommender_test = pd.DataFrame({\"user_id\": user_id, \"previous_interactions\":previous_interactions, \"recommended_app_name\":recommended_app_name, \"turns\": turns, \"recommend_indexes\":recommend_indexes})\nprint(f\"\\nnumber of rows: {len(df_recommender_test)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_recommender_test = df_recommender_test[(df_recommender_test[\"recommend_indexes\"] != -1) & (df_recommender_test[\"turns\"].apply(lambda x: len(x) > 0))]\ndf_recommender_test['user_id'] = df_recommender_test['user_id'].str.lower()\ndf_recommender_test['previous_interactions'] = df_recommender_test['previous_interactions'].str.lower()\ndf_recommender_test['recommended_app_name'] = df_recommender_test['recommended_app_name'].str.lower()\ndf_recommender_test['turns'] = df_recommender_test['turns'].apply(lambda x: [s.lower() for s in x])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Model","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"model path\"\nbos = '<|startoftext|>'\neos = '<|endoftext|>'\npad = '<|pad|>'\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token=bos, eos_token=eos, pad_token=pad, additional_special_tokens=[\"computer:\", \"human:\"],padding_side='left')\n#tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token=bos, eos_token=eos, pad_token=pad, additional_special_tokens=[\"computer:\", \"human:\", \"candidate_apps:\"],padding_side='left')\nmodel = GPT2LMHeadModel.from_pretrained(model_checkpoint).to(device)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel_max_length=512\n#model_max_length=1024","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass RecommenderItem:\n    prompt: str\n    generation: Optional[str] = None\n    \nclass recommenderDataset(Dataset):\n    def __init__(self, data: List[RecommenderItem]):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx) -> RecommenderItem:\n        return self.data[idx]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_test = []\nfor _, row in df_recommender_test.iterrows():\n    if row[\"previous_interactions\"] is not None:\n        prompt = bos + row[\"previous_interactions\"]\n    else:\n        prompt = bos + \"None\"\n    items_test.append(RecommenderItem(prompt, ', ' + row[\"recommended_app_name\"] + eos))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n  for i in range(0, len(list_of_elements), batch_size):\n    yield list_of_elements[i:i+batch_size]\n\ndef evaluate_recommender(dataset, model, tokenizer, batch_size=8, device=device, threshold=70):\n  prompt_arr = [data.prompt for data in dataset]\n  generation_arr = [data.generation for data in dataset]\n  prompt_batches = list(chunk(prompt_arr, batch_size))\n  generation_batches = list(chunk(generation_arr, batch_size))\n  max_length=480\n  generation_length = 32\n  print(len(dataset))\n  correctly_predicted = []\n\n  for prompt_batch, generation_batch in tqdm(zip(prompt_batches, generation_batches), total = len(generation_batches)):\n\n    inputs = tokenizer(prompt_batch, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n\n    generations_predicted = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device),\n                            max_new_tokens=generation_length,\n                            num_beams=8,\n                            eos_token_id=tokenizer.eos_token_id,\n                            pad_token_id=tokenizer.pad_token_id,\n                            bos_token_id=tokenizer.bos_token_id) # length_penalty=0.8, Set length_penalty to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer sequences.\n\n    generations_predicted = generations_predicted[:, max_length:] # we only need the generation part, not the prompt part.\n    decoded_generations = [tokenizer.decode(generation, skip_special_tokens=True, clean_up_tokenization_spaces=True).replace(\" app.\", \"\")  for generation in generations_predicted]\n    generation_batch = [generation.replace(\" app.\", \"\") for generation in generation_batch]\n    \n    correctly_predicted.extend([1 if fuzz.ratio(predicted, ground_truth) > threshold else 0 for predicted, ground_truth in zip(decoded_generations, generation_batch)])\n\n\n  return correctly_predicted","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correctly_predicted = evaluate_recommender(recommenderDataset(items_test), model, tokenizer, batch_size=4, device=device,threshold=95)\nsuccess_rate = sum(correctly_predicted) / len(correctly_predicted)\nprint(\"success_rate: \", success_rate)","metadata":{},"execution_count":null,"outputs":[]}]}