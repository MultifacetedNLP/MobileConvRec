{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from fuzzywuzzy import fuzz\n",
    "import evaluate\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import top_k_accuracy_score, ndcg_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/goodreads/splits/train.jsonl\"\n",
    "df_recommender_train = pd.read_json(input_file, lines=True)\n",
    "for _, row in df_recommender_train.iterrows():\n",
    "    row[\"recommended_book\"][\"book_name\"] = row[\"recommended_book\"][\"book_name\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_previous_interactions</th>\n",
       "      <th>recommended_book</th>\n",
       "      <th>negative_recommended_book</th>\n",
       "      <th>turns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>[{'book_name': 'Web Standards Solutions: The M...</td>\n",
       "      <td>{'book_name': 'pride and prejudice', 'book_id'...</td>\n",
       "      <td>[{'book_name': 'the fiery cross (outlander, #5...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>[{'book_name': 'Web Standards Solutions: The M...</td>\n",
       "      <td>{'book_name': 'the tipping point: how little t...</td>\n",
       "      <td>[{'book_name': 'night (the night trilogy #1)',...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>[{'book_name': 'Web Standards Solutions: The M...</td>\n",
       "      <td>{'book_name': 'surely you're joking, mr. feynm...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0ccc92b2cef99f22af2b28307400dd19</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'book_name': 'the great gatsby', 'book_id': '...</td>\n",
       "      <td>[{'book_name': 'to kill a mockingbird', 'book_...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5b1a46a337306e95d09bf3d1a7d2289e</td>\n",
       "      <td>[{'book_name': 'The Valley of Horses (Earth's ...</td>\n",
       "      <td>{'book_name': 'the secret life of bees', 'book...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>e55909b0cc4b0ebf426b7e49185ab502</td>\n",
       "      <td>[{'book_name': 'Eve (Eve, #1)', 'book_id': '92...</td>\n",
       "      <td>{'book_name': 'a monster calls', 'book_id': '8...</td>\n",
       "      <td>[{'book_name': 'city of heavenly fire (the mor...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3f8351b28cb6060aba4872c3c783fa01</td>\n",
       "      <td>[{'book_name': 'Bully (Fall Away, #1)', 'book_...</td>\n",
       "      <td>{'book_name': 'mr. president (white house, #1)...</td>\n",
       "      <td>[{'book_name': 'real (real, #1)', 'book_id': '...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>b70962b73d3a34b919b3da0cfd71cd13</td>\n",
       "      <td>[{'book_name': 'Kitty Steals the Show (Kitty N...</td>\n",
       "      <td>{'book_name': 'pet sematary', 'book_id': '2336...</td>\n",
       "      <td>[{'book_name': 'bag of bones', 'book_id': '105...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>2c8d4f471c943c1f13e34a42bd8034b3</td>\n",
       "      <td>[{'book_name': 'Every Heart', 'book_id': '2365...</td>\n",
       "      <td>{'book_name': 'elicit', 'book_id': '31737890',...</td>\n",
       "      <td>[{'book_name': 'the deep end (honey, #1)', 'bo...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>566ca6c7b7e43864c5b73c77b69763be</td>\n",
       "      <td>[{'book_name': 'High Tide', 'book_id': '260397...</td>\n",
       "      <td>{'book_name': 'death ship', 'book_id': '301617...</td>\n",
       "      <td>[{'book_name': 'if we were villains', 'book_id...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9003 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               user_id  \\\n",
       "0     8842281e1d1347389f2ab93d60773d4d   \n",
       "1     8842281e1d1347389f2ab93d60773d4d   \n",
       "2     8842281e1d1347389f2ab93d60773d4d   \n",
       "3     0ccc92b2cef99f22af2b28307400dd19   \n",
       "4     5b1a46a337306e95d09bf3d1a7d2289e   \n",
       "...                                ...   \n",
       "8998  e55909b0cc4b0ebf426b7e49185ab502   \n",
       "8999  3f8351b28cb6060aba4872c3c783fa01   \n",
       "9000  b70962b73d3a34b919b3da0cfd71cd13   \n",
       "9001  2c8d4f471c943c1f13e34a42bd8034b3   \n",
       "9002  566ca6c7b7e43864c5b73c77b69763be   \n",
       "\n",
       "                             user_previous_interactions  \\\n",
       "0     [{'book_name': 'Web Standards Solutions: The M...   \n",
       "1     [{'book_name': 'Web Standards Solutions: The M...   \n",
       "2     [{'book_name': 'Web Standards Solutions: The M...   \n",
       "3                                                    []   \n",
       "4     [{'book_name': 'The Valley of Horses (Earth's ...   \n",
       "...                                                 ...   \n",
       "8998  [{'book_name': 'Eve (Eve, #1)', 'book_id': '92...   \n",
       "8999  [{'book_name': 'Bully (Fall Away, #1)', 'book_...   \n",
       "9000  [{'book_name': 'Kitty Steals the Show (Kitty N...   \n",
       "9001  [{'book_name': 'Every Heart', 'book_id': '2365...   \n",
       "9002  [{'book_name': 'High Tide', 'book_id': '260397...   \n",
       "\n",
       "                                       recommended_book  \\\n",
       "0     {'book_name': 'pride and prejudice', 'book_id'...   \n",
       "1     {'book_name': 'the tipping point: how little t...   \n",
       "2     {'book_name': 'surely you're joking, mr. feynm...   \n",
       "3     {'book_name': 'the great gatsby', 'book_id': '...   \n",
       "4     {'book_name': 'the secret life of bees', 'book...   \n",
       "...                                                 ...   \n",
       "8998  {'book_name': 'a monster calls', 'book_id': '8...   \n",
       "8999  {'book_name': 'mr. president (white house, #1)...   \n",
       "9000  {'book_name': 'pet sematary', 'book_id': '2336...   \n",
       "9001  {'book_name': 'elicit', 'book_id': '31737890',...   \n",
       "9002  {'book_name': 'death ship', 'book_id': '301617...   \n",
       "\n",
       "                              negative_recommended_book  \\\n",
       "0     [{'book_name': 'the fiery cross (outlander, #5...   \n",
       "1     [{'book_name': 'night (the night trilogy #1)',...   \n",
       "2                                                    []   \n",
       "3     [{'book_name': 'to kill a mockingbird', 'book_...   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "8998  [{'book_name': 'city of heavenly fire (the mor...   \n",
       "8999  [{'book_name': 'real (real, #1)', 'book_id': '...   \n",
       "9000  [{'book_name': 'bag of bones', 'book_id': '105...   \n",
       "9001  [{'book_name': 'the deep end (honey, #1)', 'bo...   \n",
       "9002  [{'book_name': 'if we were villains', 'book_id...   \n",
       "\n",
       "                                                  turns  \n",
       "0     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "2     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "3     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "4     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "...                                                 ...  \n",
       "8998  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "8999  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "9000  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "9001  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "9002  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "\n",
       "[9003 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recommender_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/goodreads/splits/val.jsonl\"\n",
    "df_recommender_validation = pd.read_json(input_file, lines=True)\n",
    "for _, row in df_recommender_validation.iterrows():\n",
    "    row[\"recommended_book\"][\"book_name\"] = row[\"recommended_book\"][\"book_name\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_previous_interactions</th>\n",
       "      <th>recommended_book</th>\n",
       "      <th>negative_recommended_book</th>\n",
       "      <th>turns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a4dc04139e3341aab1646ff79128f25</td>\n",
       "      <td>[{'book_name': 'Apocalypse: The Lords of Deliv...</td>\n",
       "      <td>{'book_name': 'marriage games (the games duet,...</td>\n",
       "      <td>[{'book_name': 'aced (driven, #4)', 'book_id':...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7cb5ca2718789c2ea81b00a7e887073</td>\n",
       "      <td>[{'book_name': 'Magic Breaks (Kate Daniels, #7...</td>\n",
       "      <td>{'book_name': 'gardens of the moon (the malaza...</td>\n",
       "      <td>[{'book_name': 'stormwalker (stormwalker, #1)'...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0c2511284f1a56239d879d5f29dc95a9</td>\n",
       "      <td>[{'book_name': 'Darkfever (Fever, #1)', 'book_...</td>\n",
       "      <td>{'book_name': 'unsuitable', 'book_id': '321957...</td>\n",
       "      <td>[{'book_name': 'the ending i want', 'book_id':...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a4dc04139e3341aab1646ff79128f25</td>\n",
       "      <td>[{'book_name': 'Apocalypse: The Lords of Deliv...</td>\n",
       "      <td>{'book_name': 'american queen (new camelot tri...</td>\n",
       "      <td>[{'book_name': 'chances (lucky santangelo, #1)...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6c39913e49c45ca8682891ff8664faf9</td>\n",
       "      <td>[{'book_name': 'Mary and O'Neil', 'book_id': '...</td>\n",
       "      <td>{'book_name': 'blister', 'book_id': '30373904'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>aa1ddc7ae913cdd7360335853ec8b99b</td>\n",
       "      <td>[{'book_name': 'Let the Wind Speak', 'book_id'...</td>\n",
       "      <td>{'book_name': 'all the light we cannot see', '...</td>\n",
       "      <td>[{'book_name': 'the kite runner', 'book_id': '...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>6c39913e49c45ca8682891ff8664faf9</td>\n",
       "      <td>[{'book_name': 'Mary and O'Neil', 'book_id': '...</td>\n",
       "      <td>{'book_name': 'almost missed you', 'book_id': ...</td>\n",
       "      <td>[{'book_name': 'the thing on the doorstep and ...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>58c8f055fc71eb77eb3260faa622edc2</td>\n",
       "      <td>[{'book_name': 'Banking the Billionaire (Billi...</td>\n",
       "      <td>{'book_name': 'most valuable playboy', 'book_i...</td>\n",
       "      <td>[{'book_name': 'american queen (new camelot tr...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>b70962b73d3a34b919b3da0cfd71cd13</td>\n",
       "      <td>[{'book_name': 'Kitty Steals the Show (Kitty N...</td>\n",
       "      <td>{'book_name': 'golden son', 'book_id': '254305...</td>\n",
       "      <td>[{'book_name': 'morning star (red rising, #3)'...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>b70962b73d3a34b919b3da0cfd71cd13</td>\n",
       "      <td>[{'book_name': 'Kitty Steals the Show (Kitty N...</td>\n",
       "      <td>{'book_name': 'traitor's blade (greatcoats, #1...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1932 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               user_id  \\\n",
       "0     5a4dc04139e3341aab1646ff79128f25   \n",
       "1     b7cb5ca2718789c2ea81b00a7e887073   \n",
       "2     0c2511284f1a56239d879d5f29dc95a9   \n",
       "3     5a4dc04139e3341aab1646ff79128f25   \n",
       "4     6c39913e49c45ca8682891ff8664faf9   \n",
       "...                                ...   \n",
       "1927  aa1ddc7ae913cdd7360335853ec8b99b   \n",
       "1928  6c39913e49c45ca8682891ff8664faf9   \n",
       "1929  58c8f055fc71eb77eb3260faa622edc2   \n",
       "1930  b70962b73d3a34b919b3da0cfd71cd13   \n",
       "1931  b70962b73d3a34b919b3da0cfd71cd13   \n",
       "\n",
       "                             user_previous_interactions  \\\n",
       "0     [{'book_name': 'Apocalypse: The Lords of Deliv...   \n",
       "1     [{'book_name': 'Magic Breaks (Kate Daniels, #7...   \n",
       "2     [{'book_name': 'Darkfever (Fever, #1)', 'book_...   \n",
       "3     [{'book_name': 'Apocalypse: The Lords of Deliv...   \n",
       "4     [{'book_name': 'Mary and O'Neil', 'book_id': '...   \n",
       "...                                                 ...   \n",
       "1927  [{'book_name': 'Let the Wind Speak', 'book_id'...   \n",
       "1928  [{'book_name': 'Mary and O'Neil', 'book_id': '...   \n",
       "1929  [{'book_name': 'Banking the Billionaire (Billi...   \n",
       "1930  [{'book_name': 'Kitty Steals the Show (Kitty N...   \n",
       "1931  [{'book_name': 'Kitty Steals the Show (Kitty N...   \n",
       "\n",
       "                                       recommended_book  \\\n",
       "0     {'book_name': 'marriage games (the games duet,...   \n",
       "1     {'book_name': 'gardens of the moon (the malaza...   \n",
       "2     {'book_name': 'unsuitable', 'book_id': '321957...   \n",
       "3     {'book_name': 'american queen (new camelot tri...   \n",
       "4     {'book_name': 'blister', 'book_id': '30373904'...   \n",
       "...                                                 ...   \n",
       "1927  {'book_name': 'all the light we cannot see', '...   \n",
       "1928  {'book_name': 'almost missed you', 'book_id': ...   \n",
       "1929  {'book_name': 'most valuable playboy', 'book_i...   \n",
       "1930  {'book_name': 'golden son', 'book_id': '254305...   \n",
       "1931  {'book_name': 'traitor's blade (greatcoats, #1...   \n",
       "\n",
       "                              negative_recommended_book  \\\n",
       "0     [{'book_name': 'aced (driven, #4)', 'book_id':...   \n",
       "1     [{'book_name': 'stormwalker (stormwalker, #1)'...   \n",
       "2     [{'book_name': 'the ending i want', 'book_id':...   \n",
       "3     [{'book_name': 'chances (lucky santangelo, #1)...   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "1927  [{'book_name': 'the kite runner', 'book_id': '...   \n",
       "1928  [{'book_name': 'the thing on the doorstep and ...   \n",
       "1929  [{'book_name': 'american queen (new camelot tr...   \n",
       "1930  [{'book_name': 'morning star (red rising, #3)'...   \n",
       "1931                                                 []   \n",
       "\n",
       "                                                  turns  \n",
       "0     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "2     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "3     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "4     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "...                                                 ...  \n",
       "1927  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1928  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1929  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1930  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1931  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "\n",
       "[1932 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recommender_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path = \"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", additional_special_tokens=[\"computer:\", \"human:\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9003 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9003/9003 [00:42<00:00, 213.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 35\n",
      "len(prompt_train): 8968\n",
      "len(recommend_train): 8968\n"
     ]
    }
   ],
   "source": [
    "prompt_train = []\n",
    "recommend_train = []\n",
    "not_founds = 0\n",
    "\n",
    "for _, row in tqdm(df_recommender_train.iterrows(), total=len(df_recommender_train)):\n",
    "    prompt = \"\"\n",
    "    found = False\n",
    "    recommended = row[\"recommended_book\"][\"book_name\"]\n",
    "    \n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        if \"COMPUTER\" in turn:\n",
    "            computer = turn[\"COMPUTER\"]\n",
    "        \n",
    "            if fuzz.partial_ratio(recommended, computer.lower()) >= 95:\n",
    "                prompt += \"computer: I would recommend the \"\n",
    "                prompt_train.append(prompt)\n",
    "                recommend_train.append(recommended)\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                prompt += \"computer: \"+ computer + \"\\n\"\n",
    "        \n",
    "        if \"HUMAN\" in turn:\n",
    "            human = turn[\"HUMAN\"]\n",
    "            prompt += \"human: \" + human + \"\\n\"\n",
    "    \n",
    "    if not found:\n",
    "        not_founds += 1\n",
    "\n",
    "print(f\"Could not find {not_founds}\")\n",
    "print(f\"len(prompt_train): {len(prompt_train)}\")\n",
    "print(f\"len(recommend_train): {len(recommend_train)}\")\n",
    "\n",
    "            \n",
    "            \n",
    "prompt_encodings = tokenizer(prompt_train, padding='max_length', max_length=1024, truncation=True, return_tensors='pt')\n",
    "recommend_encodings = tokenizer(recommend_train, padding='max_length', max_length=32, truncation=True, return_tensors='pt')\n",
    "\n",
    "labels = recommend_encodings['input_ids']\n",
    "labels[labels == tokenizer.pad_token_id] = IGNORE_INDEX\n",
    "\n",
    "dataset = {\n",
    "    'input_ids': prompt_encodings['input_ids'],\n",
    "    'attention_mask': prompt_encodings['attention_mask'],\n",
    "    'labels': labels,\n",
    "}\n",
    "dataset_train = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1932/1932 [00:08<00:00, 224.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 5\n",
      "len(prompt_validation): 1927\n",
      "len(recommend_validation): 1927\n"
     ]
    }
   ],
   "source": [
    "prompt_validation = []\n",
    "recommend_validation = []\n",
    "not_founds = 0\n",
    "\n",
    "for _, row in tqdm(df_recommender_validation.iterrows(), total=len(df_recommender_validation)):\n",
    "    prompt = \"\"\n",
    "    found = False\n",
    "    recommended = row[\"recommended_book\"][\"book_name\"]\n",
    "    \n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        if \"COMPUTER\" in turn:\n",
    "            computer = turn[\"COMPUTER\"]\n",
    "            \n",
    "            if fuzz.partial_ratio(recommended, computer.lower()) >= 95:\n",
    "                prompt += \"computer: I would recommend the \"\n",
    "                prompt_validation.append(prompt)\n",
    "                recommend_validation.append(recommended)\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                prompt += \"computer: \"+ computer + \"\\n\"\n",
    "        \n",
    "        if \"HUMAN\" in turn:\n",
    "            human = turn[\"HUMAN\"]\n",
    "            prompt += \"human: \" + human + \"\\n\"\n",
    "    \n",
    "    if not found:\n",
    "        not_founds += 1\n",
    "        \n",
    "print(f\"Could not find {not_founds}\")\n",
    "print(f\"len(prompt_validation): {len(prompt_validation)}\")\n",
    "print(f\"len(recommend_validation): {len(recommend_validation)}\")\n",
    "            \n",
    "            \n",
    "prompt_encodings = tokenizer(prompt_validation, padding='max_length', max_length=1024, truncation=True, return_tensors='pt')\n",
    "recommend_encodings = tokenizer(recommend_validation, padding='max_length', max_length=32, truncation=True, return_tensors='pt')\n",
    "\n",
    "labels = recommend_encodings['input_ids']\n",
    "labels[labels == tokenizer.pad_token_id] = IGNORE_INDEX\n",
    "\n",
    "dataset = {\n",
    "    'input_ids': prompt_encodings['input_ids'],\n",
    "    'attention_mask': prompt_encodings['attention_mask'],\n",
    "    'labels': labels,\n",
    "}\n",
    "dataset_validation = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    input_ids, attention_mask, labels,  = [], [], []\n",
    "    for sample in batch:\n",
    "        input_ids.append(sample['input_ids'])\n",
    "        attention_mask.append(sample['attention_mask'])\n",
    "        labels.append(sample['labels'])\n",
    "    max_encoder_len = max(sum(x) for x in attention_mask)\n",
    "    max_decoder_len = max(sum([0 if item == IGNORE_INDEX else 1 for item in x]) for x in labels)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids)[:, :max_encoder_len],\n",
    "        'attention_mask': torch.tensor(attention_mask)[:, :max_encoder_len],\n",
    "        'labels': torch.tensor(labels)[:, :max_decoder_len]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/models/new_models/goodreads/T5_recommender\",\n",
    "    num_train_epochs=5,\n",
    "    # logging_steps=500,\n",
    "    # logging_dir=self.cfg.logging_dir,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=0.3,#self.cfg.save_steps,\n",
    "    eval_steps=0.3, #self.cfg.eval_steps,\n",
    "    save_total_limit=3,\n",
    "    gradient_accumulation_steps=3, #gradient_accumulation_steps,\n",
    "    per_device_train_batch_size=4, #train_batch_size,\n",
    "    per_device_eval_batch_size=4, #self.cfg.eval_batch_size,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    # dataloader_drop_last=True,\n",
    "    disable_tqdm=False,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_train,\n",
    "        eval_dataset=dataset_validation,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3735' max='3735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3735/3735 30:32, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1121</td>\n",
       "      <td>2.950400</td>\n",
       "      <td>3.065450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2242</td>\n",
       "      <td>2.666100</td>\n",
       "      <td>2.961706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3363</td>\n",
       "      <td>2.500900</td>\n",
       "      <td>2.932088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "trainer.train() # resume_from_checkpoint=True\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and test it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/goodreads/splits/test.jsonl\"\n",
    "df_recommender_test = pd.read_json(input_file, lines=True)\n",
    "for _, row in df_recommender_test.iterrows():\n",
    "    row[\"recommended_book\"][\"book_name\"] = row[\"recommended_book\"][\"book_name\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_training_path = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/goodreads/goodreads_df.csv\"\n",
    "\n",
    "all_apps = []\n",
    "with open(apps_training_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        all_apps.append(row[\"title\"].lower())\n",
    "        \n",
    "all_apps = list(set(all_apps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_creator(row):\n",
    "    np.random.seed(row.name)\n",
    "    selected_values = np.random.choice(np.setdiff1d( all_apps, [row[\"recommended_book\"][\"book_name\"]]), 24, replace=False) # filter_candidate_apps(row[\"recommended_book\"][\"book_name\"])\n",
    "    random_position = np.random.randint(0, len(selected_values) + 1)\n",
    "    \n",
    "    return np.insert(selected_values, random_position, row[\"recommended_book\"][\"book_name\"]) \n",
    "\n",
    "df_recommender_test['candidate'] = df_recommender_test.apply(lambda row: candidate_creator(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 7\n",
      "Number of prompt: 1916\n",
      "Number of generations: 1916\n",
      "Number of candidate apps: 1916\n",
      "Number of true candidate indexes: 1916\n"
     ]
    }
   ],
   "source": [
    "prompt_test = []\n",
    "recommend_test = []\n",
    "candidate_books = []\n",
    "true_candidate_indexes = []\n",
    "not_founds = 0\n",
    "for _, row in df_recommender_test.iterrows():\n",
    "    candidates = []\n",
    "    for index, candidate_book in enumerate(row[\"candidate\"].tolist()):\n",
    "        candidates.append(candidate_book)\n",
    "        if candidate_book == row[\"recommended_book\"][\"book_name\"]:\n",
    "            true_candidate_index = index\n",
    "    prompt = \"\"\n",
    "    \n",
    "    found = False\n",
    "    recommended = row[\"recommended_book\"][\"book_name\"]\n",
    "    \n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        computer = turn[\"COMPUTER\"]\n",
    "        \n",
    "        if fuzz.partial_ratio(recommended, computer.lower()) >= 95:\n",
    "            prompt += \"computer: I would recommend the \"\n",
    "            prompt_test.append(prompt)\n",
    "            recommend_test.append(recommended)\n",
    "            candidate_books.append(candidates)\n",
    "            true_candidate_indexes.append(true_candidate_index)\n",
    "            found = True\n",
    "            break\n",
    "        else:\n",
    "            prompt += \"computer: \"+ computer + \"\\n\"\n",
    "        \n",
    "        if \"HUMAN\" in turn:\n",
    "            human = turn[\"HUMAN\"]\n",
    "            prompt += \"human: \" + human + \"\\n\"\n",
    "    \n",
    "    if not found:\n",
    "        not_founds += 1\n",
    "\n",
    "print(f\"Could not find {not_founds}\")\n",
    "print(f\"Number of prompt: {len(prompt_test)}\")\n",
    "print(f\"Number of generations: {len(recommend_test)}\")\n",
    "print(f\"Number of candidate apps: {len(candidate_books)}\")\n",
    "print(f\"Number of true candidate indexes: {len(true_candidate_indexes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/models/new_models/goodreads/T5_recommender\")\n",
    "model.eval()\n",
    "model = model.to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", additional_special_tokens=[\"computer:\", \"human:\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i:i+batch_size]\n",
    "\n",
    "def evaluate_recommender(prompt_test, recommend_test, model, tokenizer, batch_size=8, threshold=70):\n",
    "  prompt_batches = list(chunk(prompt_test, batch_size))\n",
    "  generation_batches = list(chunk(recommend_test, batch_size))\n",
    "\n",
    "  correctly_predicted = []\n",
    "  for prompt_batch, generation_batch in tqdm(zip(prompt_batches, generation_batches), total = len(generation_batches)):\n",
    "\n",
    "    inputs = tokenizer(prompt_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\n",
    "    generations_predicted = model.generate(input_ids=inputs[\"input_ids\"].to('cuda'), attention_mask=inputs[\"attention_mask\"].to('cuda'),\n",
    "                            max_new_tokens=32,\n",
    "                            num_beams=8,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id) # length_penalty=0.8, Set length_penalty to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer sequences.\n",
    "\n",
    "    decoded_generations = [tokenizer.decode(generation, skip_special_tokens=True, clean_up_tokenization_spaces=True) for generation in generations_predicted]\n",
    "    generation_batch = [generation for generation in generation_batch]\n",
    "    \n",
    "    correctly_predicted.extend([1 if fuzz.ratio(predicted, ground_truth) > threshold else 0 for predicted, ground_truth in zip(decoded_generations, generation_batch)])\n",
    "\n",
    "  return correctly_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/240 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [04:45<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_rate:  0.009916492693110648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correctly_predicted = evaluate_recommender(prompt_test, recommend_test, model, tokenizer, batch_size=8, threshold=70)\n",
    "success_rate = sum(correctly_predicted) / len(correctly_predicted)\n",
    "print(\"success_rate: \", success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i:i+batch_size]\n",
    "    \n",
    "def convert_to_sublists(numbers, sublist_size):\n",
    "    return [numbers[i:i+sublist_size] for i in range(0, len(numbers), sublist_size)]\n",
    "\n",
    "def recommender_rank(prompts, candidate_apps, model, tokenizer, batch_size=8):\n",
    "  model.eval()\n",
    "  encoder_max_length = 1024\n",
    "  decoder_max_length = 32\n",
    "  prompts_tokenized = tokenizer(prompts, max_length=encoder_max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "  \n",
    "  input_ids_decoder = []\n",
    "  attention_mask_decoder = []\n",
    "  input_ids_encoder = []\n",
    "  attention_mask_encoder  = []\n",
    "  for index, candidate_app_elements in enumerate(candidate_apps):\n",
    "    candidate_app_elements = [tokenizer.pad_token+element for element in candidate_app_elements] # adding pad token to the beginning of each candidate app\n",
    "    candidate_apps_tokenized = tokenizer(candidate_app_elements, max_length=decoder_max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    for candidate_app_index in range(len(candidate_app_elements)):\n",
    "      input_ids_decoder.append(candidate_apps_tokenized[\"input_ids\"][candidate_app_index])\n",
    "      attention_mask_decoder.append(candidate_apps_tokenized[\"attention_mask\"][candidate_app_index])\n",
    "      input_ids_encoder.append(prompts_tokenized[\"input_ids\"][index])\n",
    "      attention_mask_encoder.append(prompts_tokenized[\"attention_mask\"][index])\n",
    "  \n",
    "  input_ids_encoder_batches = list(chunk(input_ids_encoder, batch_size))\n",
    "  attention_mask_encoder_batches = list(chunk(attention_mask_encoder, batch_size))\n",
    "  input_ids_decoder_batches = list(chunk(input_ids_decoder, batch_size))\n",
    "  attention_mask_decoder_batches = list(chunk(attention_mask_decoder, batch_size))\n",
    "  \n",
    "\n",
    "  scores = []\n",
    "  for input_ids_encoder_batch, attention_mask_encoder_batch, input_ids_decoder_batch, attention_mask_decoder_batch in tqdm(zip(input_ids_encoder_batches, attention_mask_encoder_batches, input_ids_decoder_batches, attention_mask_decoder_batches), total = len(input_ids_encoder_batches)):\n",
    "    decoder_input_ids = torch.stack(input_ids_decoder_batch).to(\"cuda\")\n",
    "    decoder_attention_mask = torch.stack(attention_mask_decoder_batch).to(\"cuda\")\n",
    "    input_ids = torch.stack(input_ids_encoder_batch).to(\"cuda\")\n",
    "    attention_mask = torch.stack(attention_mask_encoder_batch).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "      model_output = model(decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, \n",
    "                           input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logprobs = F.log_softmax(model_output[\"logits\"], dim=-1)[:, :-1, :] # remove the eos token\n",
    "    output_tokens = decoder_input_ids[:, 1:] # remove the bos token\n",
    "        \n",
    "    tokens_logprobs = torch.gather(logprobs, 2, output_tokens[:, :, None]).squeeze(-1).to(torch.float32)\n",
    "        \n",
    "    mask = torch.ones(tokens_logprobs.shape, dtype=torch.bool, device=\"cuda\")\n",
    "    for i, _output in enumerate(output_tokens):\n",
    "      for j, _token in enumerate(_output):\n",
    "        if _token == tokenizer.pad_token_id:\n",
    "          mask[i, j] = False\n",
    "              \n",
    "    score = (tokens_logprobs * mask).sum(-1) / mask.sum(-1)\n",
    "    scores.extend(score.to('cpu').tolist())\n",
    "    \n",
    "  # batch_input_representations = torch.cat(batch_input_representations)\n",
    "  \n",
    "  scores = convert_to_sublists(scores, len(candidate_apps[0]))\n",
    "                \n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5988/5988 [22:33<00:00,  4.42it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = recommender_rank(prompt_test, candidate_books, model, tokenizer, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampled Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.07985386221294363),\n",
       " np.float64(0.1289144050104384),\n",
       " np.float64(0.16805845511482254),\n",
       " np.float64(0.2150313152400835),\n",
       " np.float64(0.2588726513569937),\n",
       " np.float64(0.2980167014613779),\n",
       " np.float64(0.3376826722338205),\n",
       " np.float64(0.3695198329853862),\n",
       " np.float64(0.40970772442588727),\n",
       " np.float64(0.4384133611691023)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that is the one\n",
    "[top_k_accuracy_score(true_candidate_indexes, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_relevance = [[1 if item == index else 0 for item in range(len(candidate_books[0]))] for index in true_candidate_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.07985386221294363),\n",
       " np.float64(0.11080761839024895),\n",
       " np.float64(0.13037964344244102),\n",
       " np.float64(0.1506097531640513),\n",
       " np.float64(0.16756989711379114),\n",
       " np.float64(0.18151328909348932),\n",
       " np.float64(0.19473527935097018),\n",
       " np.float64(0.20477878534467034),\n",
       " np.float64(0.21687654613074886),\n",
       " np.float64(0.22517433603027073)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that is the one\n",
    "[ndcg_score(true_relevance, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[top_k_accuracy_score(true_candidate_indexes, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_relevance = [[1 if item == index else 0 for item in range(len(candidate_books[0]))] for index in true_candidate_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ndcg_score(true_relevance, scores, k=k) for k in range(1, 11)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
