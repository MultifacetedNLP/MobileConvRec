{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from fuzzywuzzy import fuzz\n",
    "import evaluate\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import top_k_accuracy_score, ndcg_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/yelp/splits/train.jsonl\"\n",
    "df_recommender_train = pd.read_json(input_file, lines=True)\n",
    "for _, row in df_recommender_train.iterrows():\n",
    "    row[\"recommended_place\"][\"place_name\"] = row[\"recommended_place\"][\"place_name\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_training_path = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/yelp/yelp_df.csv\"\n",
    "\n",
    "all_apps = []\n",
    "with open(apps_training_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        all_apps.append(row[\"name\"].lower())\n",
    "        \n",
    "all_apps = list(set(all_apps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/yelp/splits/val.jsonl\"\n",
    "df_recommender_validation = pd.read_json(input_file, lines=True)\n",
    "for _, row in df_recommender_validation.iterrows():\n",
    "    row[\"recommended_place\"][\"place_name\"] = row[\"recommended_place\"][\"place_name\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_existing_length = max(len(item) for item in all_apps)  # Max length in current array\n",
    "new_dtype = f'<U{max_existing_length}'\n",
    "\n",
    "def candidate_creator(row):\n",
    "    selected_values = np.random.choice(np.setdiff1d(all_apps, [row[\"recommended_place\"][\"place_name\"]]), 24, replace=False).astype(new_dtype)\n",
    "    random_position = np.random.randint(0, len(selected_values) + 1)\n",
    "    \n",
    "    return np.insert(selected_values, random_position, row[\"recommended_place\"][\"place_name\"]) \n",
    "\n",
    "df_recommender_train['candidate'] = df_recommender_train.apply(candidate_creator, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_existing_length = max(len(item) for item in all_apps)  # Max length in current array\n",
    "new_dtype = f'<U{max_existing_length}'\n",
    "\n",
    "def candidate_creator(row):\n",
    "    selected_values = np.random.choice(np.setdiff1d(all_apps, [row[\"recommended_place\"][\"place_name\"]]), 24, replace=False).astype(new_dtype)\n",
    "    random_position = np.random.randint(0, len(selected_values) + 1)\n",
    "    \n",
    "    return np.insert(selected_values, random_position, row[\"recommended_place\"][\"place_name\"]) \n",
    "\n",
    "df_recommender_validation['candidate'] = df_recommender_validation.apply(candidate_creator, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path = \"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", additional_special_tokens=[\"computer:\", \"human:\", \"candidate_apps:\", \"previous_interactions:\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_previous_interactions</th>\n",
       "      <th>recommended_place</th>\n",
       "      <th>negative_recommended_place</th>\n",
       "      <th>turns</th>\n",
       "      <th>candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_BHTC7nyCBoZcfiiD5cOXg</td>\n",
       "      <td>[{'place_name': 'Amish Country Store', 'busine...</td>\n",
       "      <td>{'place_name': 'laziz', 'business_id': 'wS5eU8...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[zia's on the hill, manayunk diner, south jers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y5qEpAfJQaOjjG8ZJWrp6w</td>\n",
       "      <td>[{'place_name': 'Trader Joe's', 'business_id':...</td>\n",
       "      <td>{'place_name': 'vincente's restaurant', 'busin...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[tavern 17, stonewood grill &amp; tavern, fred's w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6s-g2vFu12OemhiK3FJuOQ</td>\n",
       "      <td>[{'place_name': 'Rouge', 'business_id': '4tU9h...</td>\n",
       "      <td>{'place_name': 'cafe saigon', 'business_id': '...</td>\n",
       "      <td>[{'place_name': 'Pho Ha Saigon', 'business_id'...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[sap sap lao cafe, festiva, sabrina's café, id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zqu-tMR9CHjA44sU7I5bXA</td>\n",
       "      <td>[{'place_name': 'Caribbean Delight', 'business...</td>\n",
       "      <td>{'place_name': 'morimoto', 'business_id': '6_T...</td>\n",
       "      <td>[{'place_name': 'Zento Contemporary Japanese C...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[tortilla press cantina, a&amp;w restaurant, ocean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sZUC9GbwvDHWrHl0D_d2dA</td>\n",
       "      <td>[{'place_name': 'Saturn Club', 'business_id': ...</td>\n",
       "      <td>{'place_name': 'silk city diner &amp; lounge', 'bu...</td>\n",
       "      <td>[{'place_name': 'Parc', 'business_id': 'j-qtdD...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[michael's grill, marlene's original breakfast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>WwulXySQN8t2hwqH_yWurA</td>\n",
       "      <td>[{'place_name': 'McDonald's', 'business_id': '...</td>\n",
       "      <td>{'place_name': '3 in 1 restaurant', 'business_...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[taqueria doña maria, 3 in 1 restaurant, taque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>61yERKmEa_0mkZYy_3PNAg</td>\n",
       "      <td>[{'place_name': 'Mesa Verde', 'business_id': '...</td>\n",
       "      <td>{'place_name': 'cajun kitchen cafe', 'business...</td>\n",
       "      <td>[{'place_name': 'State &amp; Fig', 'business_id': ...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[cajun kitchen cafe, ms. kelli's karaoke bar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>3CwMwbY9UgP-qvTDxNs_0g</td>\n",
       "      <td>[{'place_name': 'Reed's Dairy - Meridian', 'bu...</td>\n",
       "      <td>{'place_name': 'modern bar and restaurant', 'b...</td>\n",
       "      <td>[{'place_name': 'Sid's Garage', 'business_id':...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[modern bar and restaurant, rickshaw lounge, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>wXdbkFZsfDR7utJvbWElyA</td>\n",
       "      <td>[{'place_name': 'Chin Brothers Restaurant', 'b...</td>\n",
       "      <td>{'place_name': 'ale emporium', 'business_id': ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[modo mio taverna, ale emporium, jamaican styl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9742</th>\n",
       "      <td>gNfjkQdaUJ3TvntckOLiRA</td>\n",
       "      <td>[{'place_name': 'Montecito Pet Shop', 'busines...</td>\n",
       "      <td>{'place_name': 'la cocina mexican restaurant',...</td>\n",
       "      <td>[{'place_name': 'Yona Redz', 'business_id': 'W...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "      <td>[ice dreammm shop, the oasis, bay breeze car w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9743 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user_id  \\\n",
       "0     _BHTC7nyCBoZcfiiD5cOXg   \n",
       "1     y5qEpAfJQaOjjG8ZJWrp6w   \n",
       "2     6s-g2vFu12OemhiK3FJuOQ   \n",
       "3     Zqu-tMR9CHjA44sU7I5bXA   \n",
       "4     sZUC9GbwvDHWrHl0D_d2dA   \n",
       "...                      ...   \n",
       "9738  WwulXySQN8t2hwqH_yWurA   \n",
       "9739  61yERKmEa_0mkZYy_3PNAg   \n",
       "9740  3CwMwbY9UgP-qvTDxNs_0g   \n",
       "9741  wXdbkFZsfDR7utJvbWElyA   \n",
       "9742  gNfjkQdaUJ3TvntckOLiRA   \n",
       "\n",
       "                             user_previous_interactions  \\\n",
       "0     [{'place_name': 'Amish Country Store', 'busine...   \n",
       "1     [{'place_name': 'Trader Joe's', 'business_id':...   \n",
       "2     [{'place_name': 'Rouge', 'business_id': '4tU9h...   \n",
       "3     [{'place_name': 'Caribbean Delight', 'business...   \n",
       "4     [{'place_name': 'Saturn Club', 'business_id': ...   \n",
       "...                                                 ...   \n",
       "9738  [{'place_name': 'McDonald's', 'business_id': '...   \n",
       "9739  [{'place_name': 'Mesa Verde', 'business_id': '...   \n",
       "9740  [{'place_name': 'Reed's Dairy - Meridian', 'bu...   \n",
       "9741  [{'place_name': 'Chin Brothers Restaurant', 'b...   \n",
       "9742  [{'place_name': 'Montecito Pet Shop', 'busines...   \n",
       "\n",
       "                                      recommended_place  \\\n",
       "0     {'place_name': 'laziz', 'business_id': 'wS5eU8...   \n",
       "1     {'place_name': 'vincente's restaurant', 'busin...   \n",
       "2     {'place_name': 'cafe saigon', 'business_id': '...   \n",
       "3     {'place_name': 'morimoto', 'business_id': '6_T...   \n",
       "4     {'place_name': 'silk city diner & lounge', 'bu...   \n",
       "...                                                 ...   \n",
       "9738  {'place_name': '3 in 1 restaurant', 'business_...   \n",
       "9739  {'place_name': 'cajun kitchen cafe', 'business...   \n",
       "9740  {'place_name': 'modern bar and restaurant', 'b...   \n",
       "9741  {'place_name': 'ale emporium', 'business_id': ...   \n",
       "9742  {'place_name': 'la cocina mexican restaurant',...   \n",
       "\n",
       "                             negative_recommended_place  \\\n",
       "0                                                    []   \n",
       "1                                                    []   \n",
       "2     [{'place_name': 'Pho Ha Saigon', 'business_id'...   \n",
       "3     [{'place_name': 'Zento Contemporary Japanese C...   \n",
       "4     [{'place_name': 'Parc', 'business_id': 'j-qtdD...   \n",
       "...                                                 ...   \n",
       "9738                                                 []   \n",
       "9739  [{'place_name': 'State & Fig', 'business_id': ...   \n",
       "9740  [{'place_name': 'Sid's Garage', 'business_id':...   \n",
       "9741                                                 []   \n",
       "9742  [{'place_name': 'Yona Redz', 'business_id': 'W...   \n",
       "\n",
       "                                                  turns  \\\n",
       "0     [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "1     [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "2     [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "3     [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "4     [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "...                                                 ...   \n",
       "9738  [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "9739  [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "9740  [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "9741  [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "9742  [{'turn': 1, 'is_rec': False, 'user_accept_rec...   \n",
       "\n",
       "                                              candidate  \n",
       "0     [zia's on the hill, manayunk diner, south jers...  \n",
       "1     [tavern 17, stonewood grill & tavern, fred's w...  \n",
       "2     [sap sap lao cafe, festiva, sabrina's café, id...  \n",
       "3     [tortilla press cantina, a&w restaurant, ocean...  \n",
       "4     [michael's grill, marlene's original breakfast...  \n",
       "...                                                 ...  \n",
       "9738  [taqueria doña maria, 3 in 1 restaurant, taque...  \n",
       "9739  [cajun kitchen cafe, ms. kelli's karaoke bar, ...  \n",
       "9740  [modern bar and restaurant, rickshaw lounge, j...  \n",
       "9741  [modo mio taverna, ale emporium, jamaican styl...  \n",
       "9742  [ice dreammm shop, the oasis, bay breeze car w...  \n",
       "\n",
       "[9743 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recommender_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 49\n",
      "len(prompt_train): 9694\n",
      "len(recommend_train): 9694\n"
     ]
    }
   ],
   "source": [
    "prompt_train = []\n",
    "recommend_train = []\n",
    "not_founds = 0\n",
    "\n",
    "for _, row in df_recommender_train.iterrows():\n",
    "    if len(row[\"user_previous_interactions\"]) > 0:\n",
    "        row[\"user_previous_interactions\"] = random.sample(row[\"user_previous_interactions\"], min(3, len(row[\"user_previous_interactions\"])))\n",
    "        previous_interactions_items = [previous_interactions[\"place_name\"] for previous_interactions in row[\"user_previous_interactions\"]]\n",
    "        prompt = \"previous_interactions: \" + \", \".join(previous_interactions_items) + \"\\n\"\n",
    "    else:\n",
    "        prompt = \"previous_interactions: No previous interactions\" + \"\\n\"\n",
    "    found = False\n",
    "    recommended = row[\"recommended_place\"][\"place_name\"]\n",
    "    \n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        computer = turn[\"COMPUTER\"]\n",
    "        \n",
    "        if fuzz.partial_ratio(recommended, computer.lower()) >= 95:\n",
    "            prompt += \"candidate: \"\n",
    "            for app in row[\"candidate\"]:\n",
    "                prompt += \"'\" + app + \"', \"\n",
    "            prompt += \"\\n\"\n",
    "            prompt += \"computer: I would recommend the \"\n",
    "            prompt_train.append(prompt)\n",
    "            recommend_train.append(recommended)\n",
    "            found = True\n",
    "            break\n",
    "        else:\n",
    "            prompt += \"computer: \"+ computer + \"\\n\"\n",
    "        \n",
    "        if \"HUMAN\" in turn:\n",
    "            human = turn[\"HUMAN\"]\n",
    "            prompt += \"human: \" + human + \"\\n\"\n",
    "            \n",
    "    if not found:\n",
    "        not_founds += 1\n",
    "            \n",
    "print(f\"Could not find {not_founds}\")\n",
    "print(f\"len(prompt_train): {len(prompt_train)}\")\n",
    "print(f\"len(recommend_train): {len(recommend_train)}\")\n",
    "            \n",
    "tokenizer.truncation_side = 'left'\n",
    "prompt_encodings = tokenizer(prompt_train, padding='max_length', max_length=1400, truncation=True, return_tensors='pt')\n",
    "recommend_encodings = tokenizer(recommend_train, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "\n",
    "labels = recommend_encodings['input_ids']\n",
    "labels[labels == tokenizer.pad_token_id] = IGNORE_INDEX\n",
    "\n",
    "dataset = {\n",
    "    'input_ids': prompt_encodings['input_ids'],\n",
    "    'attention_mask': prompt_encodings['attention_mask'],\n",
    "    'labels': labels,\n",
    "}\n",
    "dataset_train = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 19\n",
      "len(prompt_validation): 2068\n",
      "len(recommend_validation): 2068\n"
     ]
    }
   ],
   "source": [
    "prompt_validation = []\n",
    "recommend_validation = []\n",
    "not_founds = 0\n",
    "\n",
    "for _, row in df_recommender_validation.iterrows():\n",
    "    if len(row[\"user_previous_interactions\"]) > 0:\n",
    "        row[\"user_previous_interactions\"] = random.sample(row[\"user_previous_interactions\"], min(3, len(row[\"user_previous_interactions\"])))\n",
    "        previous_interactions_items = [previous_interactions[\"place_name\"] for previous_interactions in row[\"user_previous_interactions\"]]\n",
    "        prompt = \"previous_interactions: \" + \", \".join(previous_interactions_items) + \"\\n\"\n",
    "    else:\n",
    "        prompt = \"previous_interactions: No previous interactions\" + \"\\n\"\n",
    "        \n",
    "    found = False\n",
    "    recommended = row[\"recommended_place\"][\"place_name\"]\n",
    "    \n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        computer = turn[\"COMPUTER\"]\n",
    "        \n",
    "        if fuzz.partial_ratio(recommended, computer.lower()) >= 95:\n",
    "            prompt += \"candidate: \"\n",
    "            for app in row[\"candidate\"]:\n",
    "                prompt += \"'\" + app + \"', \"\n",
    "            prompt += \"\\n\"\n",
    "            prompt += \"computer: I would recommend the \"\n",
    "            prompt_validation.append(prompt)\n",
    "            recommend_validation.append(recommended)\n",
    "            found = True\n",
    "            break\n",
    "        else:\n",
    "            prompt += \"computer: \"+ computer + \"\\n\"\n",
    "        \n",
    "        if \"HUMAN\" in turn:\n",
    "            human = turn[\"HUMAN\"]\n",
    "            prompt += \"human: \" + human + \"\\n\"\n",
    "            \n",
    "    if not found:\n",
    "        not_founds += 1\n",
    "        \n",
    "print(f\"Could not find {not_founds}\")\n",
    "print(f\"len(prompt_validation): {len(prompt_validation)}\")\n",
    "print(f\"len(recommend_validation): {len(recommend_validation)}\")\n",
    "            \n",
    "tokenizer.truncation_side = 'left'\n",
    "prompt_encodings = tokenizer(prompt_validation, padding='max_length', max_length=1400, truncation=True, return_tensors='pt')\n",
    "recommend_encodings = tokenizer(recommend_validation, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "\n",
    "labels = recommend_encodings['input_ids']\n",
    "labels[labels == tokenizer.pad_token_id] = IGNORE_INDEX\n",
    "\n",
    "dataset = {\n",
    "    'input_ids': prompt_encodings['input_ids'],\n",
    "    'attention_mask': prompt_encodings['attention_mask'],\n",
    "    'labels': labels,\n",
    "}\n",
    "dataset_validation = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    input_ids, attention_mask, labels,  = [], [], []\n",
    "    for sample in batch:\n",
    "        input_ids.append(sample['input_ids'])\n",
    "        attention_mask.append(sample['attention_mask'])\n",
    "        labels.append(sample['labels'])\n",
    "    max_encoder_len = max(sum(x) for x in attention_mask)\n",
    "    max_decoder_len = max(sum([0 if item == IGNORE_INDEX else 1 for item in x]) for x in labels)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids)[:, :max_encoder_len],\n",
    "        'attention_mask': torch.tensor(attention_mask)[:, :max_encoder_len],\n",
    "        'labels': torch.tensor(labels)[:, :max_decoder_len]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/models/new_models/yelp/T5_previous_interactions_candidate_apps\",\n",
    "    num_train_epochs=5,\n",
    "    # logging_steps=500,\n",
    "    # logging_dir=self.cfg.logging_dir,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=0.3,#self.cfg.save_steps,\n",
    "    eval_steps=0.3, #self.cfg.eval_steps,\n",
    "    save_total_limit=3,\n",
    "    gradient_accumulation_steps=4, #gradient_accumulation_steps,\n",
    "    per_device_train_batch_size=3, #train_batch_size,\n",
    "    per_device_eval_batch_size=3, #self.cfg.eval_batch_size,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    # dataloader_drop_last=True,\n",
    "    disable_tqdm=False,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_train,\n",
    "        eval_dataset=dataset_validation,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4040' max='4040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4040/4040 51:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1212</td>\n",
       "      <td>0.716600</td>\n",
       "      <td>0.515867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2424</td>\n",
       "      <td>0.552600</td>\n",
       "      <td>0.481940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3636</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.475215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and test it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/yelp/splits/test.jsonl\"\n",
    "df_recommender_test = pd.read_json(input_file, lines=True)\n",
    "for _, row in df_recommender_test.iterrows():\n",
    "    row[\"recommended_place\"][\"place_name\"] = row[\"recommended_place\"][\"place_name\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_training_path = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/yelp/yelp_df.csv\"\n",
    "\n",
    "all_apps = []\n",
    "with open(apps_training_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        all_apps.append(row[\"name\"].lower())\n",
    "        \n",
    "all_apps = list(set(all_apps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_existing_length = max(len(item) for item in all_apps)  # Max length in current array\n",
    "new_dtype = f'<U{max_existing_length}'\n",
    "\n",
    "def candidate_creator(row):\n",
    "    np.random.seed(row.name)\n",
    "    selected_values = np.random.choice(np.setdiff1d(all_apps, [row[\"recommended_place\"][\"place_name\"]]), 24, replace=False).astype(new_dtype) # filter_candidate_apps(row[\"recommended_product\"][\"product_name\"]) \n",
    "    random_position = np.random.randint(0, len(selected_values) + 1)\n",
    "    \n",
    "    return np.insert(selected_values, random_position, row[\"recommended_place\"][\"place_name\"]) \n",
    "\n",
    "df_recommender_test['candidate'] = df_recommender_test.apply(lambda row: candidate_creator(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 11\n",
      "Number of prompt: 2078\n",
      "Number of generations: 2078\n",
      "Number of candidate apps: 2078\n",
      "Number of true candidate indexes: 2078\n"
     ]
    }
   ],
   "source": [
    "prompt_test = []\n",
    "recommend_test = []\n",
    "candidate_apps = []\n",
    "true_candidate_indexes = []\n",
    "not_founds = 0\n",
    "\n",
    "for _, row in df_recommender_test.iterrows():\n",
    "    # creating candidate apps\n",
    "    candidates = []\n",
    "    for index, candidate_app in enumerate(row[\"candidate\"].tolist()):\n",
    "        candidates.append(candidate_app)\n",
    "        if candidate_app == row[\"recommended_place\"][\"place_name\"]:\n",
    "            true_candidate_index = index\n",
    "            \n",
    "    if len(row[\"user_previous_interactions\"]) > 0:\n",
    "        row[\"user_previous_interactions\"] = random.sample(row[\"user_previous_interactions\"], min(3, len(row[\"user_previous_interactions\"])))\n",
    "        previous_interactions_items = [previous_interactions[\"place_name\"] for previous_interactions in row[\"user_previous_interactions\"]]\n",
    "        prompt = \"previous_interactions: \" + \", \".join(previous_interactions_items) + \"\\n\"\n",
    "    else:\n",
    "        prompt = \"previous_interactions: No previous interactions\" + \"\\n\"\n",
    "    found = False\n",
    "    recommended = row[\"recommended_place\"][\"place_name\"]\n",
    "    \n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        computer = turn[\"COMPUTER\"]\n",
    "        \n",
    "        if fuzz.partial_ratio(recommended, computer.lower()) >= 95:\n",
    "            prompt += \"candidate: \"\n",
    "            for app in row[\"candidate\"]:\n",
    "                prompt += \"'\" + app + \"', \"\n",
    "            prompt += \"\\n\"\n",
    "            prompt += \"computer: I would recommend the \"\n",
    "            prompt_test.append(prompt)\n",
    "            recommend_test.append(recommended)\n",
    "            candidate_apps.append(candidates)\n",
    "            true_candidate_indexes.append(true_candidate_index)\n",
    "            found = True\n",
    "            break\n",
    "        else:\n",
    "            prompt += \"computer: \"+ computer + \"\\n\"\n",
    "        \n",
    "        if \"HUMAN\" in turn:\n",
    "            human = turn[\"HUMAN\"]\n",
    "            prompt += \"human: \" + human + \"\\n\"\n",
    "    \n",
    "    if not found:\n",
    "        not_founds += 1\n",
    "            \n",
    "            \n",
    "print(f\"Could not find {not_founds}\")\n",
    "print(f\"Number of prompt: {len(prompt_test)}\")\n",
    "print(f\"Number of generations: {len(recommend_test)}\")\n",
    "print(f\"Number of candidate apps: {len(candidate_apps)}\")\n",
    "print(f\"Number of true candidate indexes: {len(true_candidate_indexes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/models/new_models/yelp/T5_previous_interactions_candidate_apps\")\n",
    "model.eval()\n",
    "model = model.to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", additional_special_tokens=[\"computer:\", \"human:\",  \"candidate_apps:\", \"previous_interactions:\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "IGNORE_INDEX = -100\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i:i+batch_size]\n",
    "\n",
    "def evaluate_recommender(prompt_test, recommend_test, model, tokenizer, batch_size=8, threshold=70):\n",
    "  prompt_batches = list(chunk(prompt_test, batch_size))\n",
    "  generation_batches = list(chunk(recommend_test, batch_size))\n",
    "\n",
    "  correctly_predicted = []\n",
    "  for prompt_batch, generation_batch in tqdm(zip(prompt_batches, generation_batches), total = len(generation_batches)):\n",
    "\n",
    "    inputs = tokenizer(prompt_batch, max_length=1400, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\n",
    "    generations_predicted = model.generate(input_ids=inputs[\"input_ids\"].to('cuda'), attention_mask=inputs[\"attention_mask\"].to('cuda'),\n",
    "                            max_new_tokens=32,\n",
    "                            num_beams=8,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id) # length_penalty=0.8, Set length_penalty to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer sequences.\n",
    "\n",
    "    decoded_generations = [tokenizer.decode(generation, skip_special_tokens=True, clean_up_tokenization_spaces=True) for generation in generations_predicted]\n",
    "    generation_batch = [generation for generation in generation_batch]\n",
    "    \n",
    "    correctly_predicted.extend([1 if fuzz.ratio(predicted, ground_truth) > threshold else 0 for predicted, ground_truth in zip(decoded_generations, generation_batch)])\n",
    "\n",
    "  return correctly_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/260 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [05:04<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_rate:  0.43647738209817133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correctly_predicted = evaluate_recommender(prompt_test, recommend_test, model, tokenizer, batch_size=8, threshold=70)\n",
    "success_rate = sum(correctly_predicted) / len(correctly_predicted)\n",
    "print(\"success_rate: \", success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i:i+batch_size]\n",
    "    \n",
    "def convert_to_sublists(numbers, sublist_size):\n",
    "    return [numbers[i:i+sublist_size] for i in range(0, len(numbers), sublist_size)]\n",
    "\n",
    "def recommender_rank(prompts, candidate_apps, model, tokenizer, batch_size=8):\n",
    "  model.eval()\n",
    "  encoder_max_length = 1400\n",
    "  decoder_max_length = 32\n",
    "  tokenizer.truncation_side='left'\n",
    "  prompts_tokenized = tokenizer(prompts, max_length=encoder_max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "  tokenizer.truncation_side='right'\n",
    "  input_ids_decoder = []\n",
    "  attention_mask_decoder = []\n",
    "  input_ids_encoder = []\n",
    "  attention_mask_encoder  = []\n",
    "  for index, candidate_app_elements in enumerate(candidate_apps):\n",
    "    candidate_app_elements = [tokenizer.pad_token+element for element in candidate_app_elements] # adding pad token to the beginning of each candidate app\n",
    "    candidate_apps_tokenized = tokenizer(candidate_app_elements, max_length=decoder_max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    for candidate_app_index in range(len(candidate_app_elements)):\n",
    "      input_ids_decoder.append(candidate_apps_tokenized[\"input_ids\"][candidate_app_index])\n",
    "      attention_mask_decoder.append(candidate_apps_tokenized[\"attention_mask\"][candidate_app_index])\n",
    "      input_ids_encoder.append(prompts_tokenized[\"input_ids\"][index])\n",
    "      attention_mask_encoder.append(prompts_tokenized[\"attention_mask\"][index])\n",
    "  \n",
    "  input_ids_encoder_batches = list(chunk(input_ids_encoder, batch_size))\n",
    "  attention_mask_encoder_batches = list(chunk(attention_mask_encoder, batch_size))\n",
    "  input_ids_decoder_batches = list(chunk(input_ids_decoder, batch_size))\n",
    "  attention_mask_decoder_batches = list(chunk(attention_mask_decoder, batch_size))\n",
    "  \n",
    "\n",
    "  scores = []\n",
    "  for input_ids_encoder_batch, attention_mask_encoder_batch, input_ids_decoder_batch, attention_mask_decoder_batch in tqdm(zip(input_ids_encoder_batches, attention_mask_encoder_batches, input_ids_decoder_batches, attention_mask_decoder_batches), total = len(input_ids_encoder_batches)):\n",
    "    decoder_input_ids = torch.stack(input_ids_decoder_batch).to(\"cuda\")\n",
    "    decoder_attention_mask = torch.stack(attention_mask_decoder_batch).to(\"cuda\")\n",
    "    input_ids = torch.stack(input_ids_encoder_batch).to(\"cuda\")\n",
    "    attention_mask = torch.stack(attention_mask_encoder_batch).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "      model_output = model(decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, \n",
    "                           input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logprobs = F.log_softmax(model_output[\"logits\"], dim=-1)[:, :-1, :] # remove the eos token\n",
    "    output_tokens = decoder_input_ids[:, 1:] # remove the bos token\n",
    "        \n",
    "    tokens_logprobs = torch.gather(logprobs, 2, output_tokens[:, :, None]).squeeze(-1).to(torch.float32)\n",
    "        \n",
    "    mask = torch.ones(tokens_logprobs.shape, dtype=torch.bool, device=\"cuda\")\n",
    "    for i, _output in enumerate(output_tokens):\n",
    "      for j, _token in enumerate(_output):\n",
    "        if _token == tokenizer.pad_token_id:\n",
    "          mask[i, j] = False\n",
    "              \n",
    "    score = (tokens_logprobs * mask).sum(-1) / mask.sum(-1)\n",
    "    scores.extend(score.to('cpu').tolist())\n",
    "    \n",
    "  # batch_input_representations = torch.cat(batch_input_representations)\n",
    "  \n",
    "  scores = convert_to_sublists(scores, len(candidate_apps[0]))\n",
    "  \n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6494/6494 [37:43<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = recommender_rank(prompt_test, candidate_apps, model, tokenizer, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.4292589027911453),\n",
       " np.float64(0.5567853705486044),\n",
       " np.float64(0.6246390760346487),\n",
       " np.float64(0.6626564003849855),\n",
       " np.float64(0.6891241578440809),\n",
       " np.float64(0.7151106833493744),\n",
       " np.float64(0.7430221366698749),\n",
       " np.float64(0.7714148219441771),\n",
       " np.float64(0.7925890279114534),\n",
       " np.float64(0.8055822906641001)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the one\n",
    "[top_k_accuracy_score(true_candidate_indexes, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_relevance = [[1 if item == index else 0 for item in range(len(candidate_apps[0]))] for index in true_candidate_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.4292589027911453),\n",
       " np.float64(0.5097191456671973),\n",
       " np.float64(0.5436459984102195),\n",
       " np.float64(0.5600191688085825),\n",
       " np.float64(0.5702582950828363),\n",
       " np.float64(0.5795148822357877),\n",
       " np.float64(0.588818700009288),\n",
       " np.float64(0.5977755949709618),\n",
       " np.float64(0.6041496661014792),\n",
       " np.float64(0.6079055613423757)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# this is the one\n",
    "[ndcg_score(true_relevance, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[top_k_accuracy_score(true_candidate_indexes, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_relevance = [[1 if item == index else 0 for item in range(len(candidate_apps[0]))] for index in true_candidate_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ndcg_score(true_relevance, scores, k=k) for k in range(1, 11)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
