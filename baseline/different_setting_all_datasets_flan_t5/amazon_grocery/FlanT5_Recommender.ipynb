{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from fuzzywuzzy import fuzz\n",
    "import evaluate\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import top_k_accuracy_score, ndcg_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_five_words(sentence):\n",
    "    words = sentence.split()  # Split the sentence into a list of words\n",
    "    return \" \".join(words[:10])  # Join the first 5 words back into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/amazon_grocery/splits/train.jsonl\"\n",
    "df_recommender_train = pd.read_json(input_file, lines=True)\n",
    "for _, row in df_recommender_train.iterrows():\n",
    "    row[\"recommended_product\"][\"product_name\"] = row[\"recommended_product\"][\"product_name\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_previous_interactions</th>\n",
       "      <th>recommended_product</th>\n",
       "      <th>negative_recommended_product</th>\n",
       "      <th>turns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHG34BLPBFJAZLKSZTRSVPBYMNQA</td>\n",
       "      <td>[{'product_name': 'Ateco Food Coloring Kit, 6 ...</td>\n",
       "      <td>{'product_name': 'ateco 1112 12-color food col...</td>\n",
       "      <td>[{'product_name': 'McCormick Nature's Inspirat...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFOSCJK6ARZVTE42E2BOYOCVSPWQ</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'product_name': 'welpac panko bread crumbs', ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGWDYYVVWM3DC3CASUZKXK67G6IA</td>\n",
       "      <td>[{'product_name': 'Tazo 149901 Tea Bags, Calm ...</td>\n",
       "      <td>{'product_name': 'sucanat sugar, 1 lb.', 'pare...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFNVEHQGMJHJFM4I7ZJPRXCZFMZA</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'product_name': 'senseo coffee pods, kona ble...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG357TH7OYHATTMDUYIVEE2M4UQ</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'product_name': 'better than bouillon premium...</td>\n",
       "      <td>[{'product_name': 'Herb-Ox Sodium Free Bouillo...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8401</th>\n",
       "      <td>AGQ7M4IO7E2S2GXHQY2SNFG2ATOA</td>\n",
       "      <td>[{'product_name': 'Bar Harbor All Sauce, White...</td>\n",
       "      <td>{'product_name': 'american gumball company ass...</td>\n",
       "      <td>[{'product_name': 'Large 1\" Blue Gumballs - 2 ...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8402</th>\n",
       "      <td>AH7PP6TAJYHFIMUC4WLFCG44AJSQ</td>\n",
       "      <td>[{'product_name': 'STONE COLD JO: 12 oz, Cold ...</td>\n",
       "      <td>{'product_name': 'breville nespresso vertuo co...</td>\n",
       "      <td>[{'product_name': 'Dark Chocolate Covered Roas...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>AGEAKAPK6PVQHRBE5XNWTJPRQCKQ</td>\n",
       "      <td>[{'product_name': 'Sunbest Natural - Jumbo Roa...</td>\n",
       "      <td>{'product_name': 'iya's all natural cassava fl...</td>\n",
       "      <td>[{'product_name': 'Terrasoul Superfoods Organi...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8404</th>\n",
       "      <td>AHCWAEALGQUJSPXMEZIB3EADS2RA</td>\n",
       "      <td>[{'product_name': 'Dress My Cupcake Decorating...</td>\n",
       "      <td>{'product_name': 'tate's bake shop white choco...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405</th>\n",
       "      <td>AHA7K2OWRPUHCCOQWYUVDLLXCI7A</td>\n",
       "      <td>[{'product_name': 'Hershey's Caramel Syrup, 22...</td>\n",
       "      <td>{'product_name': 'nut cravings gourmet collect...</td>\n",
       "      <td>[{'product_name': 'Dried Fruit &amp; Mixed Nuts Gi...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8406 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           user_id  \\\n",
       "0     AHG34BLPBFJAZLKSZTRSVPBYMNQA   \n",
       "1     AFOSCJK6ARZVTE42E2BOYOCVSPWQ   \n",
       "2     AGWDYYVVWM3DC3CASUZKXK67G6IA   \n",
       "3     AFNVEHQGMJHJFM4I7ZJPRXCZFMZA   \n",
       "4     AFG357TH7OYHATTMDUYIVEE2M4UQ   \n",
       "...                            ...   \n",
       "8401  AGQ7M4IO7E2S2GXHQY2SNFG2ATOA   \n",
       "8402  AH7PP6TAJYHFIMUC4WLFCG44AJSQ   \n",
       "8403  AGEAKAPK6PVQHRBE5XNWTJPRQCKQ   \n",
       "8404  AHCWAEALGQUJSPXMEZIB3EADS2RA   \n",
       "8405  AHA7K2OWRPUHCCOQWYUVDLLXCI7A   \n",
       "\n",
       "                             user_previous_interactions  \\\n",
       "0     [{'product_name': 'Ateco Food Coloring Kit, 6 ...   \n",
       "1                                                    []   \n",
       "2     [{'product_name': 'Tazo 149901 Tea Bags, Calm ...   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "8401  [{'product_name': 'Bar Harbor All Sauce, White...   \n",
       "8402  [{'product_name': 'STONE COLD JO: 12 oz, Cold ...   \n",
       "8403  [{'product_name': 'Sunbest Natural - Jumbo Roa...   \n",
       "8404  [{'product_name': 'Dress My Cupcake Decorating...   \n",
       "8405  [{'product_name': 'Hershey's Caramel Syrup, 22...   \n",
       "\n",
       "                                    recommended_product  \\\n",
       "0     {'product_name': 'ateco 1112 12-color food col...   \n",
       "1     {'product_name': 'welpac panko bread crumbs', ...   \n",
       "2     {'product_name': 'sucanat sugar, 1 lb.', 'pare...   \n",
       "3     {'product_name': 'senseo coffee pods, kona ble...   \n",
       "4     {'product_name': 'better than bouillon premium...   \n",
       "...                                                 ...   \n",
       "8401  {'product_name': 'american gumball company ass...   \n",
       "8402  {'product_name': 'breville nespresso vertuo co...   \n",
       "8403  {'product_name': 'iya's all natural cassava fl...   \n",
       "8404  {'product_name': 'tate's bake shop white choco...   \n",
       "8405  {'product_name': 'nut cravings gourmet collect...   \n",
       "\n",
       "                           negative_recommended_product  \\\n",
       "0     [{'product_name': 'McCormick Nature's Inspirat...   \n",
       "1                                                    []   \n",
       "2                                                    []   \n",
       "3                                                    []   \n",
       "4     [{'product_name': 'Herb-Ox Sodium Free Bouillo...   \n",
       "...                                                 ...   \n",
       "8401  [{'product_name': 'Large 1\" Blue Gumballs - 2 ...   \n",
       "8402  [{'product_name': 'Dark Chocolate Covered Roas...   \n",
       "8403  [{'product_name': 'Terrasoul Superfoods Organi...   \n",
       "8404                                                 []   \n",
       "8405  [{'product_name': 'Dried Fruit & Mixed Nuts Gi...   \n",
       "\n",
       "                                                  turns  \n",
       "0     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "2     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "3     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "4     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "...                                                 ...  \n",
       "8401  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "8402  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "8403  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "8404  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "8405  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "\n",
       "[8406 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recommender_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/amazon_grocery/splits/val.jsonl\"\n",
    "df_recommender_validation = pd.read_json(input_file, lines=True)\n",
    "for _, row in df_recommender_validation.iterrows():\n",
    "    row[\"recommended_product\"][\"product_name\"] = row[\"recommended_product\"][\"product_name\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_previous_interactions</th>\n",
       "      <th>recommended_product</th>\n",
       "      <th>negative_recommended_product</th>\n",
       "      <th>turns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHKGN55E2F6I3H4HIWJNBAX2VZ2Q</td>\n",
       "      <td>[{'product_name': 'Chat Masala 2.0oz By Zamour...</td>\n",
       "      <td>{'product_name': 'minor's lobster base - 1 lb....</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGRW7YJ4PIFMVLZY6WQ6BH2XXAYA</td>\n",
       "      <td>[{'product_name': 'Zint Organic Cacao Powder (...</td>\n",
       "      <td>{'product_name': 'eden organic traditional bar...</td>\n",
       "      <td>[{'product_name': 'Barley Malt Powder, 1 lb.',...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AHVECP5Y4WSYC3T26A3QHRGYLNDQ</td>\n",
       "      <td>[{'product_name': 'Pompeian USDA Organic Robus...</td>\n",
       "      <td>{'product_name': 'now foods, certified organic...</td>\n",
       "      <td>[{'product_name': 'Brazil Nuts Roasted Salted ...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEQ5A4PCOLD3DEWCE5HHFDF7IGSA</td>\n",
       "      <td>[{'product_name': 'ORGAIN Organic Strawberries...</td>\n",
       "      <td>{'product_name': 'anchor full cream milk powde...</td>\n",
       "      <td>[{'product_name': 'Nestle Nido Milk Powder, Im...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AG23NKQFFCJYUY6IAZWHJA4GLFXA</td>\n",
       "      <td>[{'product_name': '365 by Whole Foods Market, ...</td>\n",
       "      <td>{'product_name': 'daechun(choi's1) seaweed sna...</td>\n",
       "      <td>[{'product_name': 'Annie Chun's - Crispy Organ...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>AGMZTOBO3IELI54QTURQADHVFOJQ</td>\n",
       "      <td>[{'product_name': 'Organic Instant Coffee Powd...</td>\n",
       "      <td>{'product_name': 'angie's boomchickapop milk c...</td>\n",
       "      <td>[{'product_name': 'ORGANIC, 1.9 LB Heirloom Mu...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>AEAXAJACFMXIAAH4WOHRMXPSZWFA</td>\n",
       "      <td>[{'product_name': 'Timothy's World Coffee, Kon...</td>\n",
       "      <td>{'product_name': 'good nature organic chamomil...</td>\n",
       "      <td>[{'product_name': 'VAHDAM, Chai Tea Variety Sa...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>AEHQILYLFLHZ2OONHOQCIHHYL2QA</td>\n",
       "      <td>[{'product_name': 'Prima Taste Rendang Curry S...</td>\n",
       "      <td>{'product_name': 'lao gan ma (laoganma) spicy ...</td>\n",
       "      <td>[{'product_name': 'Campbell's Chunky Soup, Hot...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>AGVFUTAWFKJFHNEPRNMGENY7SYRA</td>\n",
       "      <td>[{'product_name': 'Brewer's Best Beer Ingredie...</td>\n",
       "      <td>{'product_name': 'perfectware sweet corn glaze...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>AEAXJO2Q3EOJFRP5QBIOTDNWG5EA</td>\n",
       "      <td>[{'product_name': 'Fondo di Toscana Organic Tu...</td>\n",
       "      <td>{'product_name': 'oatsome organic oat milk - b...</td>\n",
       "      <td>[{'product_name': 'Quaker Instant Oatmeal Lowe...</td>\n",
       "      <td>[{'turn': 1, 'is_rec': False, 'user_accept_rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1801 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           user_id  \\\n",
       "0     AHKGN55E2F6I3H4HIWJNBAX2VZ2Q   \n",
       "1     AGRW7YJ4PIFMVLZY6WQ6BH2XXAYA   \n",
       "2     AHVECP5Y4WSYC3T26A3QHRGYLNDQ   \n",
       "3     AEQ5A4PCOLD3DEWCE5HHFDF7IGSA   \n",
       "4     AG23NKQFFCJYUY6IAZWHJA4GLFXA   \n",
       "...                            ...   \n",
       "1796  AGMZTOBO3IELI54QTURQADHVFOJQ   \n",
       "1797  AEAXAJACFMXIAAH4WOHRMXPSZWFA   \n",
       "1798  AEHQILYLFLHZ2OONHOQCIHHYL2QA   \n",
       "1799  AGVFUTAWFKJFHNEPRNMGENY7SYRA   \n",
       "1800  AEAXJO2Q3EOJFRP5QBIOTDNWG5EA   \n",
       "\n",
       "                             user_previous_interactions  \\\n",
       "0     [{'product_name': 'Chat Masala 2.0oz By Zamour...   \n",
       "1     [{'product_name': 'Zint Organic Cacao Powder (...   \n",
       "2     [{'product_name': 'Pompeian USDA Organic Robus...   \n",
       "3     [{'product_name': 'ORGAIN Organic Strawberries...   \n",
       "4     [{'product_name': '365 by Whole Foods Market, ...   \n",
       "...                                                 ...   \n",
       "1796  [{'product_name': 'Organic Instant Coffee Powd...   \n",
       "1797  [{'product_name': 'Timothy's World Coffee, Kon...   \n",
       "1798  [{'product_name': 'Prima Taste Rendang Curry S...   \n",
       "1799  [{'product_name': 'Brewer's Best Beer Ingredie...   \n",
       "1800  [{'product_name': 'Fondo di Toscana Organic Tu...   \n",
       "\n",
       "                                    recommended_product  \\\n",
       "0     {'product_name': 'minor's lobster base - 1 lb....   \n",
       "1     {'product_name': 'eden organic traditional bar...   \n",
       "2     {'product_name': 'now foods, certified organic...   \n",
       "3     {'product_name': 'anchor full cream milk powde...   \n",
       "4     {'product_name': 'daechun(choi's1) seaweed sna...   \n",
       "...                                                 ...   \n",
       "1796  {'product_name': 'angie's boomchickapop milk c...   \n",
       "1797  {'product_name': 'good nature organic chamomil...   \n",
       "1798  {'product_name': 'lao gan ma (laoganma) spicy ...   \n",
       "1799  {'product_name': 'perfectware sweet corn glaze...   \n",
       "1800  {'product_name': 'oatsome organic oat milk - b...   \n",
       "\n",
       "                           negative_recommended_product  \\\n",
       "0                                                    []   \n",
       "1     [{'product_name': 'Barley Malt Powder, 1 lb.',...   \n",
       "2     [{'product_name': 'Brazil Nuts Roasted Salted ...   \n",
       "3     [{'product_name': 'Nestle Nido Milk Powder, Im...   \n",
       "4     [{'product_name': 'Annie Chun's - Crispy Organ...   \n",
       "...                                                 ...   \n",
       "1796  [{'product_name': 'ORGANIC, 1.9 LB Heirloom Mu...   \n",
       "1797  [{'product_name': 'VAHDAM, Chai Tea Variety Sa...   \n",
       "1798  [{'product_name': 'Campbell's Chunky Soup, Hot...   \n",
       "1799                                                 []   \n",
       "1800  [{'product_name': 'Quaker Instant Oatmeal Lowe...   \n",
       "\n",
       "                                                  turns  \n",
       "0     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "2     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "3     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "4     [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "...                                                 ...  \n",
       "1796  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1797  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1798  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1799  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "1800  [{'turn': 1, 'is_rec': False, 'user_accept_rec...  \n",
       "\n",
       "[1801 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recommender_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path = \"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", additional_special_tokens=[\"computer:\", \"human:\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8406 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8406/8406 [01:40<00:00, 83.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 113\n",
      "len(prompt_train): 8293\n",
      "len(recommend_train): 8293\n"
     ]
    }
   ],
   "source": [
    "prompt_train = []\n",
    "recommend_train = []\n",
    "not_founds = 0\n",
    "\n",
    "for _, row in tqdm(df_recommender_train.iterrows(), total=len(df_recommender_train)):\n",
    "    prompt = \"\"\n",
    "    found = False\n",
    "    recommended = get_first_five_words(row[\"recommended_product\"][\"product_name\"])\n",
    "    \n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        if \"COMPUTER\" in turn:\n",
    "            computer = turn[\"COMPUTER\"]\n",
    "        \n",
    "            if fuzz.partial_ratio(recommended, computer.lower()) >= 95:\n",
    "                prompt += \"computer: I would recommend the \"\n",
    "                prompt_train.append(prompt)\n",
    "                recommend_train.append(recommended)\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                prompt += \"computer: \"+ computer + \"\\n\"\n",
    "        \n",
    "        if \"HUMAN\" in turn:\n",
    "            human = turn[\"HUMAN\"]\n",
    "            prompt += \"human: \" + human + \"\\n\"\n",
    "    \n",
    "    if not found:\n",
    "        not_founds += 1\n",
    "\n",
    "print(f\"Could not find {not_founds}\")\n",
    "print(f\"len(prompt_train): {len(prompt_train)}\")\n",
    "print(f\"len(recommend_train): {len(recommend_train)}\")\n",
    "\n",
    "            \n",
    "            \n",
    "prompt_encodings = tokenizer(prompt_train, padding='max_length', max_length=1024, truncation=True, return_tensors='pt')\n",
    "recommend_encodings = tokenizer(recommend_train, padding='max_length', max_length=32, truncation=True, return_tensors='pt')\n",
    "\n",
    "labels = recommend_encodings['input_ids']\n",
    "labels[labels == tokenizer.pad_token_id] = IGNORE_INDEX\n",
    "\n",
    "dataset = {\n",
    "    'input_ids': prompt_encodings['input_ids'],\n",
    "    'attention_mask': prompt_encodings['attention_mask'],\n",
    "    'labels': labels,\n",
    "}\n",
    "dataset_train = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1801 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1801/1801 [00:21<00:00, 82.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 30\n",
      "len(prompt_validation): 1771\n",
      "len(recommend_validation): 1771\n"
     ]
    }
   ],
   "source": [
    "prompt_validation = []\n",
    "recommend_validation = []\n",
    "not_founds = 0\n",
    "\n",
    "for _, row in tqdm(df_recommender_validation.iterrows(), total=len(df_recommender_validation)):\n",
    "    prompt = \"\"\n",
    "    found = False\n",
    "    recommended = get_first_five_words(row[\"recommended_product\"][\"product_name\"])\n",
    "    \n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        if \"COMPUTER\" in turn:\n",
    "            computer = turn[\"COMPUTER\"]\n",
    "            \n",
    "            if fuzz.partial_ratio(recommended, computer.lower()) >= 95:\n",
    "                prompt += \"computer: I would recommend the \"\n",
    "                prompt_validation.append(prompt)\n",
    "                recommend_validation.append(recommended)\n",
    "                found = True\n",
    "                break\n",
    "            else:\n",
    "                prompt += \"computer: \"+ computer + \"\\n\"\n",
    "        \n",
    "        if \"HUMAN\" in turn:\n",
    "            human = turn[\"HUMAN\"]\n",
    "            prompt += \"human: \" + human + \"\\n\"\n",
    "    \n",
    "    if not found:\n",
    "        not_founds += 1\n",
    "        \n",
    "print(f\"Could not find {not_founds}\")\n",
    "print(f\"len(prompt_validation): {len(prompt_validation)}\")\n",
    "print(f\"len(recommend_validation): {len(recommend_validation)}\")\n",
    "            \n",
    "            \n",
    "prompt_encodings = tokenizer(prompt_validation, padding='max_length', max_length=1024, truncation=True, return_tensors='pt')\n",
    "recommend_encodings = tokenizer(recommend_validation, padding='max_length', max_length=32, truncation=True, return_tensors='pt')\n",
    "\n",
    "labels = recommend_encodings['input_ids']\n",
    "labels[labels == tokenizer.pad_token_id] = IGNORE_INDEX\n",
    "\n",
    "dataset = {\n",
    "    'input_ids': prompt_encodings['input_ids'],\n",
    "    'attention_mask': prompt_encodings['attention_mask'],\n",
    "    'labels': labels,\n",
    "}\n",
    "dataset_validation = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    input_ids, attention_mask, labels,  = [], [], []\n",
    "    for sample in batch:\n",
    "        input_ids.append(sample['input_ids'])\n",
    "        attention_mask.append(sample['attention_mask'])\n",
    "        labels.append(sample['labels'])\n",
    "    max_encoder_len = max(sum(x) for x in attention_mask)\n",
    "    max_decoder_len = max(sum([0 if item == IGNORE_INDEX else 1 for item in x]) for x in labels)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids)[:, :max_encoder_len],\n",
    "        'attention_mask': torch.tensor(attention_mask)[:, :max_encoder_len],\n",
    "        'labels': torch.tensor(labels)[:, :max_decoder_len]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/models/new_models/amazon_grocery/T5_recommender\",\n",
    "    num_train_epochs=5,\n",
    "    # logging_steps=500,\n",
    "    # logging_dir=self.cfg.logging_dir,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=0.3,#self.cfg.save_steps,\n",
    "    eval_steps=0.3, #self.cfg.eval_steps,\n",
    "    save_total_limit=3,\n",
    "    gradient_accumulation_steps=3, #gradient_accumulation_steps,\n",
    "    per_device_train_batch_size=4, #train_batch_size,\n",
    "    per_device_eval_batch_size=4, #self.cfg.eval_batch_size,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    # dataloader_drop_last=True,\n",
    "    disable_tqdm=False,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_train,\n",
    "        eval_dataset=dataset_validation,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3455' max='3455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3455/3455 31:35, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>2.526800</td>\n",
       "      <td>2.405329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2074</td>\n",
       "      <td>2.269900</td>\n",
       "      <td>2.301026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3111</td>\n",
       "      <td>2.173600</td>\n",
       "      <td>2.277305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "trainer.train() # resume_from_checkpoint=True\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and test it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/amazon_grocery/splits/test.jsonl\"\n",
    "df_recommender_test = pd.read_json(input_file, lines=True)\n",
    "for _, row in df_recommender_test.iterrows():\n",
    "    row[\"recommended_product\"][\"product_name\"] = row[\"recommended_product\"][\"product_name\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_training_path = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/dataset/amazon_grocery/grocery_df.csv\"\n",
    "\n",
    "all_apps = []\n",
    "with open(apps_training_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        all_apps.append(get_first_five_words(row[\"title\"].lower()))\n",
    "        \n",
    "all_apps = list(set(all_apps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_creator(row):\n",
    "    np.random.seed(row.name)\n",
    "    selected_values = np.random.choice(np.setdiff1d( all_apps, [get_first_five_words(row[\"recommended_product\"][\"product_name\"])]), 24, replace=False) # filter_candidate_apps(row[\"recommended_book\"][\"book_name\"])\n",
    "    random_position = np.random.randint(0, len(selected_values) + 1)\n",
    "    \n",
    "    return np.insert(selected_values, random_position, get_first_five_words(row[\"recommended_product\"][\"product_name\"])) \n",
    "\n",
    "df_recommender_test['candidate'] = df_recommender_test.apply(lambda row: candidate_creator(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 18\n",
      "Number of prompt: 1784\n",
      "Number of generations: 1784\n",
      "Number of candidate apps: 1784\n",
      "Number of true candidate indexes: 1784\n"
     ]
    }
   ],
   "source": [
    "prompt_test = []\n",
    "recommend_test = []\n",
    "candidate_books = []\n",
    "true_candidate_indexes = []\n",
    "not_founds = 0\n",
    "for _, row in df_recommender_test.iterrows():\n",
    "    candidates = []\n",
    "    for index, candidate_book in enumerate(row[\"candidate\"].tolist()):\n",
    "        candidates.append(candidate_book)\n",
    "        if candidate_book == get_first_five_words(row[\"recommended_product\"][\"product_name\"]):\n",
    "            true_candidate_index = index\n",
    "    prompt = \"\"\n",
    "    \n",
    "    found = False\n",
    "    recommended = get_first_five_words(row[\"recommended_product\"][\"product_name\"])\n",
    "    \n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        computer = turn[\"COMPUTER\"]\n",
    "        \n",
    "        if fuzz.partial_ratio(recommended, computer.lower()) >= 95:\n",
    "            prompt += \"computer: I would recommend the \"\n",
    "            prompt_test.append(prompt)\n",
    "            recommend_test.append(recommended)\n",
    "            candidate_books.append(candidates)\n",
    "            true_candidate_indexes.append(true_candidate_index)\n",
    "            found = True\n",
    "            break\n",
    "        else:\n",
    "            prompt += \"computer: \"+ computer + \"\\n\"\n",
    "        \n",
    "        if \"HUMAN\" in turn:\n",
    "            human = turn[\"HUMAN\"]\n",
    "            prompt += \"human: \" + human + \"\\n\"\n",
    "    \n",
    "    if not found:\n",
    "        not_founds += 1\n",
    "\n",
    "print(f\"Could not find {not_founds}\")\n",
    "print(f\"Number of prompt: {len(prompt_test)}\")\n",
    "print(f\"Number of generations: {len(recommend_test)}\")\n",
    "print(f\"Number of candidate apps: {len(candidate_books)}\")\n",
    "print(f\"Number of true candidate indexes: {len(true_candidate_indexes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/u-spa-d4/grad/mfe261/Projects/MobileConvRec/envs/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path = \"/u/spa-d4/grad/mfe261/Projects/MobileConvRec/models/new_models/amazon_grocery/T5_recommender\")\n",
    "model.eval()\n",
    "model = model.to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", additional_special_tokens=[\"computer:\", \"human:\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i:i+batch_size]\n",
    "\n",
    "def evaluate_recommender(prompt_test, recommend_test, model, tokenizer, batch_size=8, threshold=70):\n",
    "  prompt_batches = list(chunk(prompt_test, batch_size))\n",
    "  generation_batches = list(chunk(recommend_test, batch_size))\n",
    "\n",
    "  correctly_predicted = []\n",
    "  for prompt_batch, generation_batch in tqdm(zip(prompt_batches, generation_batches), total = len(generation_batches)):\n",
    "\n",
    "    inputs = tokenizer(prompt_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\n",
    "    generations_predicted = model.generate(input_ids=inputs[\"input_ids\"].to('cuda'), attention_mask=inputs[\"attention_mask\"].to('cuda'),\n",
    "                            max_new_tokens=32,\n",
    "                            num_beams=8,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id) # length_penalty=0.8, Set length_penalty to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer sequences.\n",
    "\n",
    "    decoded_generations = [tokenizer.decode(generation, skip_special_tokens=True, clean_up_tokenization_spaces=True) for generation in generations_predicted]\n",
    "    generation_batch = [generation for generation in generation_batch]\n",
    "    \n",
    "    correctly_predicted.extend([1 if fuzz.ratio(predicted, ground_truth) > threshold else 0 for predicted, ground_truth in zip(decoded_generations, generation_batch)])\n",
    "\n",
    "  return correctly_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [04:32<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_rate:  0.12836322869955158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correctly_predicted = evaluate_recommender(prompt_test, recommend_test, model, tokenizer, batch_size=8, threshold=70)\n",
    "success_rate = sum(correctly_predicted) / len(correctly_predicted)\n",
    "print(\"success_rate: \", success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i:i+batch_size]\n",
    "    \n",
    "def convert_to_sublists(numbers, sublist_size):\n",
    "    return [numbers[i:i+sublist_size] for i in range(0, len(numbers), sublist_size)]\n",
    "\n",
    "def recommender_rank(prompts, candidate_apps, model, tokenizer, batch_size=8):\n",
    "  model.eval()\n",
    "  encoder_max_length = 1024\n",
    "  decoder_max_length = 32\n",
    "  prompts_tokenized = tokenizer(prompts, max_length=encoder_max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "  \n",
    "  input_ids_decoder = []\n",
    "  attention_mask_decoder = []\n",
    "  input_ids_encoder = []\n",
    "  attention_mask_encoder  = []\n",
    "  for index, candidate_app_elements in enumerate(candidate_apps):\n",
    "    candidate_app_elements = [tokenizer.pad_token+element for element in candidate_app_elements] # adding pad token to the beginning of each candidate app\n",
    "    candidate_apps_tokenized = tokenizer(candidate_app_elements, max_length=decoder_max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    for candidate_app_index in range(len(candidate_app_elements)):\n",
    "      input_ids_decoder.append(candidate_apps_tokenized[\"input_ids\"][candidate_app_index])\n",
    "      attention_mask_decoder.append(candidate_apps_tokenized[\"attention_mask\"][candidate_app_index])\n",
    "      input_ids_encoder.append(prompts_tokenized[\"input_ids\"][index])\n",
    "      attention_mask_encoder.append(prompts_tokenized[\"attention_mask\"][index])\n",
    "  \n",
    "  input_ids_encoder_batches = list(chunk(input_ids_encoder, batch_size))\n",
    "  attention_mask_encoder_batches = list(chunk(attention_mask_encoder, batch_size))\n",
    "  input_ids_decoder_batches = list(chunk(input_ids_decoder, batch_size))\n",
    "  attention_mask_decoder_batches = list(chunk(attention_mask_decoder, batch_size))\n",
    "  \n",
    "\n",
    "  scores = []\n",
    "  for input_ids_encoder_batch, attention_mask_encoder_batch, input_ids_decoder_batch, attention_mask_decoder_batch in tqdm(zip(input_ids_encoder_batches, attention_mask_encoder_batches, input_ids_decoder_batches, attention_mask_decoder_batches), total = len(input_ids_encoder_batches)):\n",
    "    decoder_input_ids = torch.stack(input_ids_decoder_batch).to(\"cuda\")\n",
    "    decoder_attention_mask = torch.stack(attention_mask_decoder_batch).to(\"cuda\")\n",
    "    input_ids = torch.stack(input_ids_encoder_batch).to(\"cuda\")\n",
    "    attention_mask = torch.stack(attention_mask_encoder_batch).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "      model_output = model(decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, \n",
    "                           input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logprobs = F.log_softmax(model_output[\"logits\"], dim=-1)[:, :-1, :] # remove the eos token\n",
    "    output_tokens = decoder_input_ids[:, 1:] # remove the bos token\n",
    "        \n",
    "    tokens_logprobs = torch.gather(logprobs, 2, output_tokens[:, :, None]).squeeze(-1).to(torch.float32)\n",
    "        \n",
    "    mask = torch.ones(tokens_logprobs.shape, dtype=torch.bool, device=\"cuda\")\n",
    "    for i, _output in enumerate(output_tokens):\n",
    "      for j, _token in enumerate(_output):\n",
    "        if _token == tokenizer.pad_token_id:\n",
    "          mask[i, j] = False\n",
    "              \n",
    "    score = (tokens_logprobs * mask).sum(-1) / mask.sum(-1)\n",
    "    scores.extend(score.to('cpu').tolist())\n",
    "    \n",
    "  # batch_input_representations = torch.cat(batch_input_representations)\n",
    "  \n",
    "  scores = convert_to_sublists(scores, len(candidate_apps[0]))\n",
    "                \n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5575/5575 [20:55<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = recommender_rank(prompt_test, candidate_books, model, tokenizer, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampled Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.6266816143497758),\n",
       " np.float64(0.7320627802690582),\n",
       " np.float64(0.7797085201793722),\n",
       " np.float64(0.820627802690583),\n",
       " np.float64(0.8492152466367713),\n",
       " np.float64(0.8710762331838565),\n",
       " np.float64(0.890695067264574),\n",
       " np.float64(0.9019058295964125),\n",
       " np.float64(0.9131165919282511),\n",
       " np.float64(0.9209641255605381)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that is the one\n",
    "[top_k_accuracy_score(true_candidate_indexes, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_relevance = [[1 if item == index else 0 for item in range(len(candidate_books[0]))] for index in true_candidate_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.6266816143497758),\n",
       " np.float64(0.6931697273943016),\n",
       " np.float64(0.7169925973494585),\n",
       " np.float64(0.7346155731002196),\n",
       " np.float64(0.7456747060424626),\n",
       " np.float64(0.7534617465678063),\n",
       " np.float64(0.7600013579280454),\n",
       " np.float64(0.7635379596857329),\n",
       " np.float64(0.7669127354218762),\n",
       " np.float64(0.7691811813683169)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that is the one\n",
    "[ndcg_score(true_relevance, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[top_k_accuracy_score(true_candidate_indexes, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_relevance = [[1 if item == index else 0 for item in range(len(candidate_books[0]))] for index in true_candidate_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ndcg_score(true_relevance, scores, k=k) for k in range(1, 11)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
