{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from fuzzywuzzy import fuzz\n",
    "import evaluate\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import top_k_accuracy_score, ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_approximate_substring(substring, string, threshold=70):\n",
    "    for i in range(len(string) - len(substring) + 1):\n",
    "        window = string[i:i+len(substring)]\n",
    "        similarity_ratio = fuzz.ratio(substring, window)\n",
    "        if similarity_ratio >= threshold:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8720/8720 [00:25<00:00, 342.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8720\n",
      "8720\n",
      "8720\n",
      "8720\n",
      "\n",
      "number of rows: 8720\n"
     ]
    }
   ],
   "source": [
    "conversation_training_path = \"data/dataset_v2/training\"\n",
    "\n",
    "user_id = []\n",
    "previous_interactions = []\n",
    "recommended_app_name = []\n",
    "turns = []\n",
    "recommend_indexes = []\n",
    "\n",
    "\n",
    "# List all files in the folder\n",
    "files = os.listdir(conversation_training_path)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in tqdm(files):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(conversation_training_path, filename)\n",
    "\n",
    "    # Check if the current item is a file\n",
    "    if os.path.isfile(file_path):\n",
    "        # Open and read the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read the content of the file\n",
    "            file_content = file.read().lower()\n",
    "            \n",
    "            # finding the user id\n",
    "            index_1 = file_content.find(\"user's previous interactions\")\n",
    "            user_id.append(file_content[9:index_1].rstrip('\\n'))\n",
    "            \n",
    "            # finding the User's Previous Interactions\n",
    "            index_2 = file_content.find(\"recommended app name:\")\n",
    "            previous_interactions_arr = file_content[index_1+29:index_2].rstrip('\\n').split(\"app name:\")\n",
    "            previous_interactions_arr_filtered = []\n",
    "            for previous_interaction in previous_interactions_arr[1:]:\n",
    "                previous_interactions_arr_filtered.append(previous_interaction[:previous_interaction.find(\" | \")])\n",
    "            if len(previous_interactions_arr_filtered) > 0:\n",
    "                previous_interactions.append(\",\".join(previous_interactions_arr_filtered))\n",
    "            else:\n",
    "                previous_interactions.append(None)\n",
    "            \n",
    "            # finding recommended app name\n",
    "            index_3 = file_content[index_2:].find(\"package name\")\n",
    "            recommended = file_content[index_2+22:index_2+index_3-3].rstrip('\\n')\n",
    "            recommended_app_name.append(recommended)\n",
    "            \n",
    "            # finding each turns\n",
    "            dialog_turns = []\n",
    "            dialog_index = 0\n",
    "            COMPUTER_index = file_content.find(\"computer:\")\n",
    "            file_content = file_content[COMPUTER_index:]\n",
    "            found_recommender = False\n",
    "            while True:\n",
    "                HUMAN_index = file_content.find(\"human:\")\n",
    "                if HUMAN_index == -1:\n",
    "                    break\n",
    "                turn = file_content[:HUMAN_index].rstrip('\\n') # computer dialog\n",
    "                if (recommended in turn) and not found_recommender:\n",
    "                    recommend_indexes.append(dialog_index)\n",
    "                    found_recommender = True\n",
    "                dialog_turns.append(turn)\n",
    "                dialog_index +=1\n",
    "                file_content = file_content[HUMAN_index:]\n",
    "                \n",
    "                COMPUTER_index = file_content.find(\"computer:\")\n",
    "                turn = file_content[:COMPUTER_index].rstrip('\\n') # human dialog\n",
    "                dialog_turns.append(turn)\n",
    "                dialog_index +=1\n",
    "                file_content = file_content[COMPUTER_index:]\n",
    "                \n",
    "            if not found_recommender: # approximately finding the recommender turn\n",
    "                for i, dialog_turn in enumerate(dialog_turns):\n",
    "                    if is_approximate_substring(recommended, dialog_turn):\n",
    "                        recommend_indexes.append(i)\n",
    "                        found_recommender = True\n",
    "                        break\n",
    "                    \n",
    "            if not found_recommender:\n",
    "                recommend_indexes.append(-1)\n",
    "                        \n",
    "            turns.append(dialog_turns)\n",
    "\n",
    "print(len(user_id))\n",
    "print(len(previous_interactions))\n",
    "print(len(recommended_app_name))\n",
    "print(len(recommend_indexes))\n",
    "df_recommender_train = pd.DataFrame({\"user_id\": user_id, \"previous_interactions\":previous_interactions, \"recommended_app_name\":recommended_app_name, \"turns\": turns, \"recommend_indexes\":recommend_indexes})\n",
    "print(f\"\\nnumber of rows: {len(df_recommender_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommender_train = df_recommender_train[(df_recommender_train[\"recommend_indexes\"] != -1) & (df_recommender_train[\"turns\"].apply(lambda x: len(x) > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_training_path = \"data/master_app_data_V1_true.csv\"\n",
    "\n",
    "all_apps = []\n",
    "with open(apps_training_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        all_apps.append(row[\"app_name\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_recommended_apps_names(row):\n",
    "    if row[\"recommended_app_name\"] not in all_apps:\n",
    "        for app in all_apps:\n",
    "            if fuzz.ratio(row[\"recommended_app_name\"], app) > 80:\n",
    "                return app\n",
    "        return \"uno!™\"\n",
    "    else:\n",
    "        return row[\"recommended_app_name\"]\n",
    "\n",
    "df_recommender_train['recommended_app_name'] = df_recommender_train.apply(fix_recommended_apps_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_existing_length = max(len(item) for item in all_apps)  # Max length in current array\n",
    "new_dtype = f'<U{max_existing_length}'\n",
    "\n",
    "def candidate_creator(row):\n",
    "    selected_values = np.random.choice(np.setdiff1d(all_apps, [row[\"recommended_app_name\"]]), 24, replace=False).astype(new_dtype)\n",
    "    random_position = np.random.randint(0, len(selected_values) + 1)\n",
    "    \n",
    "    return np.insert(selected_values, random_position, row[\"recommended_app_name\"]) \n",
    "\n",
    "df_recommender_train['candidate_apps'] = df_recommender_train.apply(candidate_creator, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audiomack-stream music offline'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recommender_train[\"recommended_app_name\"][874] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer: in that case, i recommend trying audiomack-stream music offline app.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recommender_train[\"turns\"][874][df_recommender_train[\"recommend_indexes\"][874]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['strawberry shortcake sweet shop', 'jetpack joyride',\n",
       "       \"five nights at freddy's 4\", 'hungry dragon', 'vlc for android',\n",
       "       'little big city 2', 'real driving school', 'dawn break -origin-',\n",
       "       'words of wonders: crossword', 'asphalt nitro',\n",
       "       'one booster - antivirus, booster, phone cleaner',\n",
       "       'spongebob: sponge on the run', 'pixellab - text on pictures',\n",
       "       'mini world: creata', 'audiomack-stream music offline',\n",
       "       'bilibili comics - read comics', 'toca life: neighborhood',\n",
       "       'car mechanic simulator 21', 'pluto tv - live tv and movies',\n",
       "       'world at arms', \"alto's adventure\", 'move people',\n",
       "       'lego® ninjago: shadow of ronin', 'asphalt 8 - car racing game',\n",
       "       'playstation app'], dtype='<U50')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recommender_train[\"candidate_apps\"][874]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1285/1285 [00:04<00:00, 300.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285\n",
      "1285\n",
      "1285\n",
      "1285\n",
      "\n",
      "number of rows: 1285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conversation_validation_path = \"data/dataset_v2/validation\"\n",
    "\n",
    "user_id = []\n",
    "previous_interactions = []\n",
    "recommended_app_name = []\n",
    "turns = []\n",
    "recommend_indexes = []\n",
    "\n",
    "\n",
    "# List all files in the folder\n",
    "files = os.listdir(conversation_validation_path)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in tqdm(files):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(conversation_validation_path, filename)\n",
    "\n",
    "    # Check if the current item is a file\n",
    "    if os.path.isfile(file_path):\n",
    "        # Open and read the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read the content of the file\n",
    "            file_content = file.read().lower()\n",
    "            \n",
    "            # finding the user id\n",
    "            index_1 = file_content.find(\"user's previous interactions\")\n",
    "            user_id.append(file_content[9:index_1].rstrip('\\n'))\n",
    "            \n",
    "            # finding the User's Previous Interactions\n",
    "            index_2 = file_content.find(\"recommended app name:\")\n",
    "            previous_interactions_arr = file_content[index_1+29:index_2].rstrip('\\n').split(\"app name:\")\n",
    "            previous_interactions_arr_filtered = []\n",
    "            for previous_interaction in previous_interactions_arr[1:]:\n",
    "                previous_interactions_arr_filtered.append(previous_interaction[:previous_interaction.find(\" | \")])\n",
    "            if len(previous_interactions_arr_filtered) > 0:\n",
    "                previous_interactions.append(\",\".join(previous_interactions_arr_filtered))\n",
    "            else:\n",
    "                previous_interactions.append(None)\n",
    "            \n",
    "            # finding recommended app name\n",
    "            index_3 = file_content[index_2:].find(\"package name\")\n",
    "            recommended = file_content[index_2+22:index_2+index_3-3].rstrip('\\n')\n",
    "            recommended_app_name.append(recommended)\n",
    "            \n",
    "            # finding each turns\n",
    "            dialog_turns = []\n",
    "            dialog_index = 0\n",
    "            COMPUTER_index = file_content.find(\"computer:\")\n",
    "            file_content = file_content[COMPUTER_index:]\n",
    "            found_recommender = False\n",
    "            while True:\n",
    "                HUMAN_index = file_content.find(\"human:\")\n",
    "                if HUMAN_index == -1:\n",
    "                    break\n",
    "                turn = file_content[:HUMAN_index].rstrip('\\n') # computer dialog\n",
    "                if (recommended in turn) and not found_recommender:\n",
    "                    recommend_indexes.append(dialog_index)\n",
    "                    found_recommender = True\n",
    "                dialog_turns.append(turn)\n",
    "                dialog_index +=1\n",
    "                file_content = file_content[HUMAN_index:]\n",
    "                \n",
    "                COMPUTER_index = file_content.find(\"computer:\")\n",
    "                turn = file_content[:COMPUTER_index].rstrip('\\n') # human dialog\n",
    "                dialog_turns.append(turn)\n",
    "                dialog_index +=1\n",
    "                file_content = file_content[COMPUTER_index:]\n",
    "                \n",
    "            if not found_recommender: # approximately finding the recommender turn\n",
    "                for i, dialog_turn in enumerate(dialog_turns):\n",
    "                    if is_approximate_substring(recommended, dialog_turn):\n",
    "                        recommend_indexes.append(i)\n",
    "                        found_recommender = True\n",
    "                        break\n",
    "                    \n",
    "            if not found_recommender:\n",
    "                recommend_indexes.append(-1)\n",
    "                        \n",
    "            turns.append(dialog_turns)\n",
    "\n",
    "print(len(user_id))\n",
    "print(len(previous_interactions))\n",
    "print(len(recommended_app_name))\n",
    "print(len(recommend_indexes))\n",
    "df_recommender_validation = pd.DataFrame({\"user_id\": user_id, \"previous_interactions\":previous_interactions, \"recommended_app_name\":recommended_app_name, \"turns\": turns, \"recommend_indexes\":recommend_indexes})\n",
    "print(f\"\\nnumber of rows: {len(df_recommender_validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommender_validation = df_recommender_validation[(df_recommender_validation[\"recommend_indexes\"] != -1) & (df_recommender_validation[\"turns\"].apply(lambda x: len(x) > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_recommended_apps_names(row):\n",
    "    if row[\"recommended_app_name\"] not in all_apps:\n",
    "        for app in all_apps:\n",
    "            if fuzz.ratio(row[\"recommended_app_name\"], app) > 80:\n",
    "                return app\n",
    "        return \"uno!™\"\n",
    "    else:\n",
    "        return row[\"recommended_app_name\"]\n",
    "\n",
    "df_recommender_validation['recommended_app_name'] = df_recommender_validation.apply(fix_recommended_apps_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_existing_length = max(len(item) for item in all_apps)  # Max length in current array\n",
    "new_dtype = f'<U{max_existing_length}'\n",
    "\n",
    "def candidate_creator(row):\n",
    "    selected_values = np.random.choice(np.setdiff1d(all_apps, [row[\"recommended_app_name\"]]), 24, replace=False).astype(new_dtype)\n",
    "    random_position = np.random.randint(0, len(selected_values) + 1)\n",
    "    \n",
    "    return np.insert(selected_values, random_position, row[\"recommended_app_name\"]) \n",
    "\n",
    "df_recommender_validation['candidate_apps'] = df_recommender_validation.apply(candidate_creator, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path = \"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", additional_special_tokens=[\"computer:\", \"human:\", \"candidate_apps:\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_validation = []\n",
    "recommend_validation = []\n",
    "for _, row in df_recommender_validation.iterrows():\n",
    "    prompt = \"\"\n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        if index < row[\"recommend_indexes\"]:\n",
    "            prompt += turn + \"\\n\"\n",
    "        elif index == row[\"recommend_indexes\"]:\n",
    "            prompt += \"candidate_apps: \"\n",
    "            for app in row[\"candidate_apps\"]:\n",
    "                prompt += \"'\" + app + \"', \"\n",
    "            prompt += \"\\n\"\n",
    "            prompt += \"computer: I would recommend the \"\n",
    "            prompt_validation.append(prompt)\n",
    "            recommend_validation.append(row[\"recommended_app_name\"] + \" app.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"error!!\")\n",
    "            \n",
    "            \n",
    "prompt_encodings = tokenizer(prompt_validation, padding='max_length', max_length=1024, truncation=True, return_tensors='pt')\n",
    "recommend_encodings = tokenizer(recommend_validation, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "\n",
    "labels = recommend_encodings['input_ids']\n",
    "labels[labels == tokenizer.pad_token_id] = IGNORE_INDEX\n",
    "\n",
    "dataset = {\n",
    "    'input_ids': prompt_encodings['input_ids'],\n",
    "    'attention_mask': prompt_encodings['attention_mask'],\n",
    "    'labels': labels,\n",
    "}\n",
    "dataset_validation = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_train = []\n",
    "recommend_train = []\n",
    "for _, row in df_recommender_train.iterrows():\n",
    "    prompt = \"\"\n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        if index < row[\"recommend_indexes\"]:\n",
    "            prompt += turn + \"\\n\"\n",
    "        elif index == row[\"recommend_indexes\"]:\n",
    "            prompt += \"candidate_apps: \"\n",
    "            for app in row[\"candidate_apps\"]:\n",
    "                prompt += \"'\" + app + \"', \"\n",
    "            prompt += \"\\n\"\n",
    "            prompt += \"computer: I would recommend the \"\n",
    "            prompt_train.append(prompt)\n",
    "            recommend_train.append(row[\"recommended_app_name\"] + \" app.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"error!!\")\n",
    "            \n",
    "            \n",
    "prompt_encodings = tokenizer(prompt_train, padding='max_length', max_length=1024, truncation=True, return_tensors='pt')\n",
    "recommend_encodings = tokenizer(recommend_train, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "\n",
    "labels = recommend_encodings['input_ids']\n",
    "labels[labels == tokenizer.pad_token_id] = IGNORE_INDEX\n",
    "\n",
    "dataset = {\n",
    "    'input_ids': prompt_encodings['input_ids'],\n",
    "    'attention_mask': prompt_encodings['attention_mask'],\n",
    "    'labels': labels,\n",
    "}\n",
    "dataset_train = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   36, 21391, 13634,     7,  1120,     5,     1,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_encodings[\"input_ids\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    input_ids, attention_mask, labels,  = [], [], []\n",
    "    for sample in batch:\n",
    "        input_ids.append(sample['input_ids'])\n",
    "        attention_mask.append(sample['attention_mask'])\n",
    "        labels.append(sample['labels'])\n",
    "    max_encoder_len = max(sum(x) for x in attention_mask)\n",
    "    max_decoder_len = max(sum([0 if item == IGNORE_INDEX else 1 for item in x]) for x in labels)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids)[:, :max_encoder_len],\n",
    "        'attention_mask': torch.tensor(attention_mask)[:, :max_encoder_len],\n",
    "        'labels': torch.tensor(labels)[:, :max_decoder_len]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/T5_candidate_apps\",\n",
    "    num_train_epochs=5,\n",
    "    # logging_steps=500,\n",
    "    # logging_dir=self.cfg.logging_dir,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=1000,#self.cfg.save_steps,\n",
    "    eval_steps=1000, #self.cfg.eval_steps,\n",
    "    save_total_limit=3,\n",
    "    gradient_accumulation_steps=3, #gradient_accumulation_steps,\n",
    "    per_device_train_batch_size=4, #train_batch_size,\n",
    "    per_device_eval_batch_size=4, #self.cfg.eval_batch_size,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    # dataloader_drop_last=True,\n",
    "    disable_tqdm=False,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_train,\n",
    "        eval_dataset=dataset_validation,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 39:25, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.090282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.067263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.063722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and test it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_approximate_substring(substring, string, threshold=70):\n",
    "    for i in range(len(string) - len(substring) + 1):\n",
    "        window = string[i:i+len(substring)]\n",
    "        similarity_ratio = fuzz.ratio(substring, window)\n",
    "        if similarity_ratio >= threshold:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2557 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2557/2557 [00:07<00:00, 352.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2557\n",
      "2557\n",
      "2557\n",
      "2557\n",
      "\n",
      "number of rows: 2557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conversation_test_path = \"data/dataset_v2/testing\"\n",
    "\n",
    "user_id = []\n",
    "previous_interactions = []\n",
    "recommended_app_name = []\n",
    "turns = []\n",
    "recommend_indexes = []\n",
    "\n",
    "\n",
    "# List all files in the folder\n",
    "files = os.listdir(conversation_test_path)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in tqdm(files):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(conversation_test_path, filename)\n",
    "\n",
    "    # Check if the current item is a file\n",
    "    if os.path.isfile(file_path):\n",
    "        # Open and read the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read the content of the file\n",
    "            file_content = file.read().lower()\n",
    "            \n",
    "            # finding the user id\n",
    "            index_1 = file_content.find(\"user's previous interactions\")\n",
    "            user_id.append(file_content[9:index_1].rstrip('\\n'))\n",
    "            \n",
    "            # finding the User's Previous Interactions\n",
    "            index_2 = file_content.find(\"recommended app name:\")\n",
    "            previous_interactions_arr = file_content[index_1+29:index_2].rstrip('\\n').split(\"app name:\")\n",
    "            previous_interactions_arr_filtered = []\n",
    "            for previous_interaction in previous_interactions_arr[1:]:\n",
    "                previous_interactions_arr_filtered.append(previous_interaction[:previous_interaction.find(\" | \")])\n",
    "            if len(previous_interactions_arr_filtered) > 0:\n",
    "                previous_interactions.append(\",\".join(previous_interactions_arr_filtered))\n",
    "            else:\n",
    "                previous_interactions.append(None)\n",
    "            \n",
    "            # finding recommended app name\n",
    "            index_3 = file_content[index_2:].find(\"package name\")\n",
    "            recommended = file_content[index_2+22:index_2+index_3-3].rstrip('\\n')\n",
    "            recommended_app_name.append(recommended)\n",
    "            \n",
    "            # finding each turns\n",
    "            dialog_turns = []\n",
    "            dialog_index = 0\n",
    "            COMPUTER_index = file_content.find(\"computer:\")\n",
    "            file_content = file_content[COMPUTER_index:]\n",
    "            found_recommender = False\n",
    "            while True:\n",
    "                HUMAN_index = file_content.find(\"human:\")\n",
    "                if HUMAN_index == -1:\n",
    "                    break\n",
    "                turn = file_content[:HUMAN_index].rstrip('\\n') # computer dialog\n",
    "                if (recommended in turn) and not found_recommender:\n",
    "                    recommend_indexes.append(dialog_index)\n",
    "                    found_recommender = True\n",
    "                dialog_turns.append(turn)\n",
    "                dialog_index +=1\n",
    "                file_content = file_content[HUMAN_index:]\n",
    "                \n",
    "                COMPUTER_index = file_content.find(\"computer:\")\n",
    "                turn = file_content[:COMPUTER_index].rstrip('\\n') # human dialog\n",
    "                dialog_turns.append(turn)\n",
    "                dialog_index +=1\n",
    "                file_content = file_content[COMPUTER_index:]\n",
    "                \n",
    "            if not found_recommender: # approximately finding the recommender turn\n",
    "                for i, dialog_turn in enumerate(dialog_turns):\n",
    "                    if is_approximate_substring(recommended, dialog_turn):\n",
    "                        recommend_indexes.append(i)\n",
    "                        found_recommender = True\n",
    "                        break\n",
    "                    \n",
    "            if not found_recommender:\n",
    "                recommend_indexes.append(-1)\n",
    "                        \n",
    "            turns.append(dialog_turns)\n",
    "\n",
    "print(len(user_id))\n",
    "print(len(previous_interactions))\n",
    "print(len(recommended_app_name))\n",
    "print(len(recommend_indexes))\n",
    "df_recommender_test = pd.DataFrame({\"user_id\": user_id, \"previous_interactions\":previous_interactions, \"recommended_app_name\":recommended_app_name, \"turns\": turns, \"recommend_indexes\":recommend_indexes})\n",
    "print(f\"\\nnumber of rows: {len(df_recommender_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommender_test = df_recommender_test[(df_recommender_test[\"recommend_indexes\"] != -1) & (df_recommender_test[\"turns\"].apply(lambda x: len(x) > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_training_path = \"data/master_app_data_V1_true.csv\"\n",
    "\n",
    "all_apps = []\n",
    "with open(apps_training_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        all_apps.append(row[\"app_name\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_recommended_apps_names(row):\n",
    "    if row[\"recommended_app_name\"] not in all_apps:\n",
    "        for app in all_apps:\n",
    "            if fuzz.ratio(row[\"recommended_app_name\"], app) > 80:\n",
    "                return app\n",
    "        return \"uno!™\"\n",
    "    else:\n",
    "        return row[\"recommended_app_name\"]\n",
    "\n",
    "df_recommender_test['recommended_app_name'] = df_recommender_test.apply(fix_recommended_apps_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_488036/766425766.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_apps = df_app.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Ensure that you've already defined 'apps_training_path' to point to your data file\n",
    "cols = ['app_name', 'app_type', 'app_category']\n",
    "df_app = pd.read_csv(apps_training_path, usecols=cols)\n",
    "df_apps = df_app.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "def filter_candidate_apps(rec_app_name):\n",
    "    candidate_apps = set()  # Using a set to avoid duplicates\n",
    "    df_rec_app = df_apps[df_apps['app_name'] == rec_app_name.lower()]\n",
    "    \n",
    "    if df_rec_app.empty:\n",
    "        print(\"No matching app found.\")\n",
    "        return []\n",
    "    \n",
    "    recommended_app_type = df_rec_app['app_type'].iloc[0]\n",
    "    recommended_app_category = df_rec_app['app_category'].iloc[0]\n",
    "    \n",
    "    df_same_category = df_apps[df_apps['app_category'] == recommended_app_category]\n",
    "    df_different_category = df_apps[df_apps['app_category'] != recommended_app_category]\n",
    "\n",
    "    # same category, same type\n",
    "    for _, row in df_same_category.iterrows():\n",
    "        if fuzz.ratio(row[\"app_type\"], recommended_app_type) > 70:\n",
    "            candidate_apps.add(row['app_name'])\n",
    "\n",
    "    # only same category\n",
    "    if len(candidate_apps) < 25:\n",
    "        for _, row in df_same_category.iterrows():\n",
    "            if len(candidate_apps) >= 25:\n",
    "                break\n",
    "            candidate_apps.add(row['app_name'])\n",
    "\n",
    "    # different category, same type\n",
    "    if len(candidate_apps) < 25:\n",
    "        for _, row in df_different_category.iterrows():\n",
    "            if len(candidate_apps) >= 25:\n",
    "                break\n",
    "            if fuzz.ratio(row[\"app_type\"], recommended_app_type) > 70:\n",
    "                candidate_apps.add(row['app_name'])\n",
    "\n",
    "    # all\n",
    "    if len(candidate_apps) < 25:\n",
    "        for _, row in df_apps.iterrows():\n",
    "            if len(candidate_apps) >= 25:\n",
    "                break\n",
    "            candidate_apps.add(row['app_name'])\n",
    "\n",
    "    return list(candidate_apps)  # Converting back to list if needed for downstream processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_existing_length = max(len(item) for item in all_apps)  # Max length in current array\n",
    "new_dtype = f'<U{max_existing_length}'\n",
    "\n",
    "def candidate_creator(row):\n",
    "    np.random.seed(row.name)\n",
    "    selected_values = np.random.choice(np.setdiff1d(all_apps, [row[\"recommended_app_name\"]]), 24, replace=False).astype(new_dtype) # filter_candidate_apps(row[\"recommended_app_name\"])\n",
    "    random_position = np.random.randint(0, len(selected_values) + 1)\n",
    "    \n",
    "    return np.insert(selected_values, random_position, row[\"recommended_app_name\"]) \n",
    "\n",
    "df_recommender_test['candidate_apps'] = df_recommender_test.apply(lambda row: candidate_creator(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_test = []\n",
    "recommend_test = []\n",
    "candidate_apps = []\n",
    "true_candidate_index = []\n",
    "for _, row in df_recommender_test.iterrows():\n",
    "    # creating candidate apps\n",
    "    candidates = []\n",
    "    for index, candidate_app in enumerate(row[\"candidate_apps\"].tolist()):\n",
    "        candidates.append(candidate_app + \" app.\")\n",
    "        if candidate_app == row[\"recommended_app_name\"]:\n",
    "            true_candidate_index.append(index)\n",
    "    candidate_apps.append(candidates)\n",
    "    prompt = \"\"\n",
    "    for index, turn in enumerate(row[\"turns\"]):\n",
    "        if index < row[\"recommend_indexes\"]:\n",
    "            prompt += turn + \"\\n\"\n",
    "        elif index == row[\"recommend_indexes\"]:\n",
    "            prompt += \"candidate_apps: \"\n",
    "            for app in row[\"candidate_apps\"]:\n",
    "                prompt += \"'\" + app + \"', \"\n",
    "            prompt += \"\\n\"\n",
    "            prompt += \"computer: I would recommend the \"\n",
    "            prompt_test.append(prompt)\n",
    "            recommend_test.append(row[\"recommended_app_name\"] + \" app.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"error!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path = \"models/T5_candidate_apps\")\n",
    "model.eval()\n",
    "model = model.to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", additional_special_tokens=[\"computer:\", \"human:\",  \"candidate_apps:\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i:i+batch_size]\n",
    "\n",
    "def evaluate_recommender(prompt_test, recommend_test, model, tokenizer, batch_size=8, threshold=70):\n",
    "  prompt_batches = list(chunk(prompt_test, batch_size))\n",
    "  generation_batches = list(chunk(recommend_test, batch_size))\n",
    "\n",
    "  correctly_predicted = []\n",
    "  for prompt_batch, generation_batch in tqdm(zip(prompt_batches, generation_batches), total = len(generation_batches)):\n",
    "\n",
    "    inputs = tokenizer(prompt_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\n",
    "    generations_predicted = model.generate(input_ids=inputs[\"input_ids\"].to('cuda'), attention_mask=inputs[\"attention_mask\"].to('cuda'),\n",
    "                            max_new_tokens=32,\n",
    "                            num_beams=8,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id) # length_penalty=0.8, Set length_penalty to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer sequences.\n",
    "\n",
    "    decoded_generations = [tokenizer.decode(generation, skip_special_tokens=True, clean_up_tokenization_spaces=True).replace(\" app.\", \"\")  for generation in generations_predicted]\n",
    "    generation_batch = [generation.replace(\" app.\", \"\") for generation in generation_batch]\n",
    "    \n",
    "    correctly_predicted.extend([1 if fuzz.ratio(predicted, ground_truth) > threshold else 0 for predicted, ground_truth in zip(decoded_generations, generation_batch)])\n",
    "\n",
    "  return correctly_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [06:38<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_rate:  0.860410094637224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correctly_predicted = evaluate_recommender(prompt_test, recommend_test, model, tokenizer, batch_size=4, threshold=95)\n",
    "success_rate = sum(correctly_predicted) / len(correctly_predicted)\n",
    "print(\"success_rate: \", success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(list_of_elements, batch_size): # using this chunk function, we can split our data to multiple batches\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i:i+batch_size]\n",
    "    \n",
    "def convert_to_sublists(numbers, sublist_size):\n",
    "    return [numbers[i:i+sublist_size] for i in range(0, len(numbers), sublist_size)]\n",
    "\n",
    "def recommender_rank(prompts, candidate_apps, model, tokenizer, batch_size=8):\n",
    "  model.eval()\n",
    "  encoder_max_length = 1024\n",
    "  decoder_max_length = 32\n",
    "  prompts_tokenized = tokenizer(prompts, max_length=encoder_max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "  \n",
    "  input_ids_decoder = []\n",
    "  attention_mask_decoder = []\n",
    "  input_ids_encoder = []\n",
    "  attention_mask_encoder  = []\n",
    "  for index, candidate_app_elements in enumerate(candidate_apps):\n",
    "    candidate_app_elements = [tokenizer.pad_token+element for element in candidate_app_elements] # adding pad token to the beginning of each candidate app\n",
    "    candidate_apps_tokenized = tokenizer(candidate_app_elements, max_length=decoder_max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    for candidate_app_index in range(len(candidate_app_elements)):\n",
    "      input_ids_decoder.append(candidate_apps_tokenized[\"input_ids\"][candidate_app_index])\n",
    "      attention_mask_decoder.append(candidate_apps_tokenized[\"attention_mask\"][candidate_app_index])\n",
    "      input_ids_encoder.append(prompts_tokenized[\"input_ids\"][index])\n",
    "      attention_mask_encoder.append(prompts_tokenized[\"attention_mask\"][index])\n",
    "  \n",
    "  input_ids_encoder_batches = list(chunk(input_ids_encoder, batch_size))\n",
    "  attention_mask_encoder_batches = list(chunk(attention_mask_encoder, batch_size))\n",
    "  input_ids_decoder_batches = list(chunk(input_ids_decoder, batch_size))\n",
    "  attention_mask_decoder_batches = list(chunk(attention_mask_decoder, batch_size))\n",
    "  \n",
    "\n",
    "  scores = []\n",
    "  for input_ids_encoder_batch, attention_mask_encoder_batch, input_ids_decoder_batch, attention_mask_decoder_batch in tqdm(zip(input_ids_encoder_batches, attention_mask_encoder_batches, input_ids_decoder_batches, attention_mask_decoder_batches), total = len(input_ids_encoder_batches)):\n",
    "    decoder_input_ids = torch.stack(input_ids_decoder_batch).to(\"cuda\")\n",
    "    decoder_attention_mask = torch.stack(attention_mask_decoder_batch).to(\"cuda\")\n",
    "    input_ids = torch.stack(input_ids_encoder_batch).to(\"cuda\")\n",
    "    attention_mask = torch.stack(attention_mask_encoder_batch).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "      model_output = model(decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, \n",
    "                           input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logprobs = F.log_softmax(model_output[\"logits\"], dim=-1)[:, :-1, :] # remove the eos token\n",
    "    output_tokens = decoder_input_ids[:, 1:] # remove the bos token\n",
    "        \n",
    "    tokens_logprobs = torch.gather(logprobs, 2, output_tokens[:, :, None]).squeeze(-1).to(torch.float32)\n",
    "        \n",
    "    mask = torch.ones(tokens_logprobs.shape, dtype=torch.bool, device=\"cuda\")\n",
    "    for i, _output in enumerate(output_tokens):\n",
    "      for j, _token in enumerate(_output):\n",
    "        if _token == tokenizer.pad_token_id:\n",
    "          mask[i, j] = False\n",
    "              \n",
    "    score = (tokens_logprobs * mask).sum(-1) / mask.sum(-1)\n",
    "    scores.extend(score.to('cpu').tolist())\n",
    "    \n",
    "  # batch_input_representations = torch.cat(batch_input_representations)\n",
    "  \n",
    "  scores = convert_to_sublists(scores, len(candidate_apps[0]))\n",
    "  \n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7925/7925 [30:41<00:00,  4.30it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = recommender_rank(prompt_test, candidate_apps, model, tokenizer, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5670347003154574,\n",
       " 0.7022870662460567,\n",
       " 0.7567034700315457,\n",
       " 0.8000788643533123,\n",
       " 0.8324132492113565,\n",
       " 0.8485804416403786,\n",
       " 0.862776025236593,\n",
       " 0.874211356466877,\n",
       " 0.8848580441640379,\n",
       " 0.8955047318611987]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[top_k_accuracy_score(true_candidate_index, scores, k=k) for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_relevance = [[1 if item == index else 0 for item in range(len(candidate_apps[0]))] for index in true_candidate_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5670347003154574,\n",
       " 0.6523694422220071,\n",
       " 0.6795776441147515,\n",
       " 0.6982584096463261,\n",
       " 0.7107670571988626,\n",
       " 0.7165259273374387,\n",
       " 0.7212577885361768,\n",
       " 0.7248652338937424,\n",
       " 0.7280702062450545,\n",
       " 0.731147789175095]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "[ndcg_score(true_relevance, scores, k=k) for k in range(1, 11)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
